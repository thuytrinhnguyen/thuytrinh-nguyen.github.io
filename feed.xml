<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-08T04:22:40+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chloe’s Blog</title><subtitle>Summarize my learning journey</subtitle><author><name>Chloe Nguyen</name></author><entry><title type="html">Simple Linear Regression: Least Squares VS. Gradient Descent</title><link href="http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression.html" rel="alternate" type="text/html" title="Simple Linear Regression: Least Squares VS. Gradient Descent" /><published>2020-11-06T00:04:00+11:00</published><updated>2020-11-06T00:04:00+11:00</updated><id>http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression</id><content type="html" xml:base="http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression.html">&lt;blockquote&gt;
  &lt;p&gt;This post sheds light on the motivation of Simple Linear Regression and discusses the two optimization methods Least Squares VS. Gradient Descent. Formulas derivation is also included at the end of the post as a bonus.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;
&lt;p&gt;It is human nature to be certain about our future and to make predictions based on existing clues. Hence, the idea of studying about the &lt;strong&gt;association&lt;/strong&gt; among subjects interests us. The first entry of the blog is dedicated to such an idea, it is about one basic concept in supervised learning: &lt;strong&gt;Simple Linear Regression&lt;/strong&gt;.&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot; id=&quot;markdown-toc-motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#least-squares-method&quot; id=&quot;markdown-toc-least-squares-method&quot;&gt;Least Squares Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-descent-method&quot; id=&quot;markdown-toc-gradient-descent-method&quot;&gt;Gradient Descent Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#model-evaluation&quot; id=&quot;markdown-toc-model-evaluation&quot;&gt;Model Evaluation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#residual-standard-error-rse&quot; id=&quot;markdown-toc-residual-standard-error-rse&quot;&gt;Residual Standard Error (RSE)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#coefficient-of-determination-r2&quot; id=&quot;markdown-toc-coefficient-of-determination-r2&quot;&gt;Coefficient of Determination (\(R^{2}\))&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#key-takeaways&quot; id=&quot;markdown-toc-key-takeaways&quot;&gt;Key Takeaways&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#limitation&quot; id=&quot;markdown-toc-limitation&quot;&gt;Limitation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bonus-least-squares---formula-derivation&quot; id=&quot;markdown-toc-bonus-least-squares---formula-derivation&quot;&gt;BONUS: Least Squares - Formula Derivation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#find-beta_0&quot; id=&quot;markdown-toc-find-beta_0&quot;&gt;Find \(\beta_0\)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#find-beta_1&quot; id=&quot;markdown-toc-find-beta_1&quot;&gt;Find \(\beta_1\)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Simple Linear Regression assumes that the independent \((x)\) and dependent \((y)\) variables have a linear relationship. Our estimation of relationship is summarized in the formula below:&lt;/p&gt;

\[\hat{y} = \beta_0 + \beta_1x\]

&lt;p&gt;\(\beta_0\) is the intercept and \(\beta_1\) is the coefficient (or slope). The name &lt;em&gt;linear regression&lt;/em&gt; suggests that the relationship resembles a straight line.&lt;/p&gt;

&lt;p style=&quot;width: 400px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/slr-regplot.png&quot; alt=&quot;SLR Regplot&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1. Visualization of a Simple Linear Regression problem&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, there can hardly be a perfect line that fits all the observations. Hence, errors are expected and we should find a way to minimize them. These errors are called &lt;strong&gt;Residual Sum of Squares (RSS)&lt;/strong&gt; which measures the total of &lt;em&gt;squared&lt;/em&gt; differences between our predictions and the true values (the differences are squared to prevent negative and positive values from cancelling out each other). The formula for RSS is as follows:&lt;/p&gt;

\[RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^{2}\]

&lt;p&gt;In this post, I will discuss two different approaches to find the “perfect” line by optimizing RSS. While Least Squares method provides a &lt;em&gt;one-shot&lt;/em&gt; solution for the problem, Gradient Descent &lt;em&gt;gradually adapts&lt;/em&gt; to it.&lt;/p&gt;

&lt;p style=&quot;100%&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/least-squares-vs-gradient-descent.png&quot; alt=&quot;Compare Least Squares and Gradient Descent&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2. Basic ideas of Least Squares method and Gradient Descent&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;least-squares-method&quot;&gt;Least Squares Method&lt;/h2&gt;

&lt;p&gt;The objective is to find values of \(\beta_0\) and \(\beta_1\) that minimize RSS. We know from calculus that by taking the first derivative of a function (e.g \(f(x)\)), and setting it to 0, we can solve for critical values that minimize \(f(x)\). This method works because the RSS function is convex, therefore, it has one global minima.&lt;/p&gt;

&lt;p style=&quot;width: 400px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/3d-rss-slr.png&quot; alt=&quot;3D plot of RSS&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 3. 3D plot of Residual Sum of Squares (RSS) (Source: &lt;a href=&quot;http://faculty.marshall.usc.edu/gareth-james/ISL/&quot;&gt;Introduction to Statistical Learning&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The details of how to derive the formulas are included in the &lt;a href=&quot;## BONUS: Least Squares - Formula Derivation&quot;&gt;BONUS Section&lt;/a&gt;. But to summarize, we can obtain \(\beta_0\) and \(\beta_1\) using:&lt;/p&gt;

\[\beta_0 = \bar{y} - \beta_1 \bar{x}\]

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}}\]

&lt;p&gt;By plugging in these two parameters, we can now find the perfect line to fit our data and predict future values: \(\hat{y}_i = \beta_0 + \beta_1 x_i\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using Least Squares method, we can calculate the exact values of \(\beta_0\) and \(\beta_1\) using all \(x\) and \(y\) values in &lt;strong&gt;one shot&lt;/strong&gt;. The idea is &lt;strong&gt;straightforward&lt;/strong&gt; and well-known. For Simple Linear Regression, applying this method is effective because we only deal with two variables. This method is suitable for smaller datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Least Squares method is &lt;strong&gt;limited to only a few loss functions&lt;/strong&gt; of certain shapes (e.g. RSS is a convex function). This method is expensive to implement on multivariate problems and mega datasets when &lt;strong&gt;complex matrix operations&lt;/strong&gt; involve (will be discussed later in Multiple Regression module). Therefore, instead of arriving at the bottom of the bowl (i.e. reaching the global minima) in one go, we can go downhill gradually. That is the inspiration of Gradient Descent.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-method&quot;&gt;Gradient Descent Method&lt;/h2&gt;
&lt;p&gt;Gradient Descent is a more general concept compared to Least Squares as it can be used on various loss functions. In the application of Simple Linear Regression, we can also use Gradient Descent to optimize RSS. The idea of Gradient Descent, as its name suggests, is to update the parameters gradually until the result reaches a certain threshold or completes a desired number of iterations (i.e. &lt;em&gt;epoches&lt;/em&gt;). You can set up a &lt;em&gt;for-loop&lt;/em&gt; in Python to run Gradient Descent using the formula:&lt;/p&gt;

\[\beta_k = \beta_k - \alpha \times \frac{\partial RSS}{\partial \beta_k}  \;\; ; \;\; k =0, 1\]

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gradient Descent works on a &lt;strong&gt;diversity of loss functions&lt;/strong&gt;. Because of its versatility, Gradient Descent is a popular optimization technique used in many machine learning applications. Another benefit is that the computation step of updating the parameter is quite simple (usually the difficult part of complex models is to calculate the derivative of the loss function with respect to the parameters). Therefore, when dealing with multivariate problems such as Multiple Regression for &lt;strong&gt;large datasets&lt;/strong&gt;, Gradient Descent has an edge over Least Square method (by not having to calculate the matrix inverses - we will discuss this in later entries).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;100%&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/learning-rate-gradient-descent.png&quot; alt=&quot;Learning rates in Gradient Descent&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 4. The choice of Learning rate can greatly affect Gradient Descent performance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Choosing the hyperparameter - &lt;strong&gt;learning rate (\(\alpha\)) can be a trouble&lt;/strong&gt; in Gradient Descent. Learning rate determines the step size of each iteration. If \(\alpha\) is too large, there will be lots of deviation and it is difficult to hit the global minima, or cannot hit the spot at all! If \(\alpha\) is too small, it will take a long time to reach the global minima. Learning rate is a hyperparameter meaning that we need to allocate a value for it. &lt;em&gt;“Is there a perfect learning rate?”&lt;/em&gt; - you may ask, it &lt;strong&gt;depends on the problem&lt;/strong&gt;, but some popular choices are 0.1, 0.05, 0.01 or smaller values, but we don’t always use a constant learning rate to run the whole model. For instance, in deep learning applications, we usually use learning rate decay (e.g. divide the learning rate in half after every 50 iterations) to prevent the gradients from exploding or vanishing. But for now, let’s look at &lt;em&gt;Fig. 4&lt;/em&gt; to see what different choices of learning rate look like in Simple Linear Regression.&lt;/p&gt;

&lt;h2 id=&quot;model-evaluation&quot;&gt;Model Evaluation&lt;/h2&gt;

&lt;p&gt;After fitting the model, we should check whether we have done a good job or if there are any mistakes. To assess the accuracy of the model, we will discuss two related quantities: Residual Standard Error (RSE) and Coefficient of Determination (\(R^{2}\)).&lt;/p&gt;

&lt;h3 id=&quot;residual-standard-error-rse&quot;&gt;Residual Standard Error (RSE)&lt;/h3&gt;
&lt;p&gt;RSE is the measurement for the &lt;em&gt;lack of fit&lt;/em&gt;. Although we have applied discussed techniques to achieve the “best” line, we would not be able to fit that line through all data points. This is because the relationship between \(X\) and \(Y\) are not perfectly linear. Comparing the &lt;em&gt;true&lt;/em&gt; equation \(y = \beta_0 + \beta_1x + \epsilon\) to our &lt;em&gt;estimated&lt;/em&gt; equation \(y = \beta_0 + \beta_1x\), it is clear that the error term \(\epsilon\) is not covered.&lt;/p&gt;

\[RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}}\]

&lt;p&gt;If \(y_i \approx \hat{y}_i\), which indicates that our prediction is close to the true value, RSE will be small and we can conclude that our model is a good fit for the data. On the other hand, if the model is not a good fit, \((y_i - \hat{y}_i)^{2}\) will be large and so as RSE.&lt;/p&gt;

&lt;h3 id=&quot;coefficient-of-determination-r2&quot;&gt;Coefficient of Determination (\(R^{2}\))&lt;/h3&gt;
&lt;p&gt;On the other end of the spectrum, \(R^{2}\) is the measurement of &lt;em&gt;model good fit&lt;/em&gt;. It can be interpreted as &lt;strong&gt;the amount of variations in \(Y\) that can be explained by \(X\)&lt;/strong&gt; using our model.&lt;/p&gt;

\[R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}}{\sum_{i=1}^{n}(y_i - \bar{y})^{2}}\]

&lt;p&gt;RSS is the variations left in \(Y\) after fitting our model, this is how much the model is lacking. TSS is the total variations (or the natural variations) in \(Y\) before fitting the model. Therefore, \(R^2\) shows how much variations in \(Y\) are resolved by our model. \(R^2\) score lies between 0 and 1. For example, \(R^2 = 0.75\) means that \(75\%\) of the variations in \(Y\) is explained by our model. A higher value of \(R^2\) indicates great goodness of fit and vice versa.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;h3 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h3&gt;
&lt;p&gt;In this post, we cover the core ideas of Simple Linear Regression and discuss two methods to optimize this model. The Least Squares method is straightforward and allows us to find \(\beta_0\) and \(\beta_1\) directly using two formulas. Gradient Descent can be used for a diversity of loss functions and can arrive at the solution gradually as long as we set a reasonable learning rate.&lt;/p&gt;

&lt;p&gt;After optimizing the model, we can assess its accuracy using RSE and \(R^2\), which represents the lack of fit and goodness of fit respectively. Thus, a good model should have low \(RSE\) and high \(R^2\).&lt;/p&gt;

&lt;h3 id=&quot;limitation&quot;&gt;Limitation&lt;/h3&gt;
&lt;p&gt;Though the simplicity of the model is a good point, Simple Linear Regression has many shortcomings. Today, we will briefly look at two major limitations. Firstly, Simple Linear Regression has an oversimplified assumption. In reality, many relationships are non-linear. Hence, forcing the relationship to be linear will not produce accurate predictions. Secondly, there can be many contributing factors to a problem and they can also have some interaction effects. Therefore, instead of having multiple Simple Linear Regression models, there should be an upgrade that takes into account the &lt;em&gt;synergy&lt;/em&gt; among variables. To answer that, we will consider &lt;strong&gt;Multiple Linear Regression&lt;/strong&gt; in the next entry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chloe’s Note&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Thank you so much for checking out my first post. Please let me know if you have any ideas on improving this post. As a bonus, I have included the details on how to derive Least Squares formulas. Hope this will be helpful. Also, stay tuned for my future posts :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;bonus-least-squares---formula-derivation&quot;&gt;BONUS: Least Squares - Formula Derivation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Find values of \(\beta_0\) and \(\beta_1\) that minimize RSS:&lt;/p&gt;

\[RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^{2}\]

&lt;h3 id=&quot;find-beta_0&quot;&gt;Find \(\beta_0\)&lt;/h3&gt;
&lt;p&gt;First, find the partial derivative of RSS with respect to \(\beta_0\) treating \(x, y\) and \(\beta_1\) as constants:&lt;/p&gt;

\[\frac{\partial RSS}{\partial \beta_0} = -2 \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i)\]

&lt;p&gt;Next, we set this value to 0 and solve for \(\beta_0\):&lt;/p&gt;

\[\frac{\partial RSS}{\partial \beta_0} = -2 \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i) = 0\]

\[\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i) = 0\]

\[\sum_{i=1}^{n}y_i - n\beta_0 - n\beta_1 \sum_{i=1}^{n}x_i = 0\]

\[\beta_0 = \frac{\sum_{i=1}^{n}y_i - \beta_1 \sum_{i=1}^{n}x_i}{n}\]

&lt;p&gt;Because:  \(\frac{\sum_{i=1}^{n}x_i}{n} = \bar{x}\)   and    \(\frac{\sum_{i=1}^{n}y_i}{n} = \bar{y}\):&lt;/p&gt;

\[{\color{Blue} {\beta_0 = \bar{y} - \beta_1 \bar{x}}}\]

&lt;h3 id=&quot;find-beta_1&quot;&gt;Find \(\beta_1\)&lt;/h3&gt;
&lt;p&gt;Similarly, we find the partial derivative of RSS with respect to \(\beta_1\) and set it to 0:&lt;/p&gt;

\[\frac{\partial RSS}{\partial \beta_1} = -2 \sum_{i=1}^{n}x_i(y_i - \beta_0 - \beta_1x_i) = 0\]

\[\sum_{i=1}^{n}x_iy_i - \beta_0 \sum_{i=1}^{n}x_i - \beta_1 \sum_{i=1}^{n}x_i^{2} = 0\]

&lt;p&gt;Plug in \(\beta_0 = \bar{y} - \beta_1 \bar{x}\):&lt;/p&gt;

\[\sum_{i=1}^{n}x_iy_i - (\bar{y} - \beta_1 \bar{x})\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^{2} = 0\]

\[\sum_{i=1}^{n}x_iy_i - \bar{y}\sum_{i=1}^{n}x_i + \beta_1 \bar{x}\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^{2} = 0\]

\[\sum_{i=1}^{n}x_i(y_i - \bar{y}) - \beta_1 \sum_{i=1}^{n}x_i (x_i - \bar{x}) = 0\]

&lt;p&gt;Solve for \(\beta_1\):&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x})}\]

&lt;p&gt;We can conclude here and use this equation to find \(\beta_1\), but the more widely used variation in books and lecture notes is:&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}} = \frac{S_{XY}}{S_{XX}}\]

&lt;p&gt;Many people find the equation above easier to remember as its numerator \((S_{XY})\) and denominator \((S_{XX})\) are the outputs of some software functions. With a few algebra adjustments, we can transform our original equation into its well-known form:&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x})}\]

\[\beta_1 = \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y}) - \sum_{i=1}^{n} \bar{x} (y_i - \bar{y}) + \sum_{i=1}^{n} \bar{x} (y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x}) - \sum_{i=1}^{n}\bar{x}(x_i - \bar{x}) + \sum_{i=1}^{n}\bar{x}(x_i - \bar{x})}\]

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y}) + \bar{x} (\sum_{i=1}^{n}y_i - n\bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2} + \bar{x} (\sum_{i=1}^{n}x_i - n\bar{x})}\]

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In the last term of the equation in the numerator, (1) \(\bar{x}\) is a number so it can come out of the sum and (2) \(\sum_{i=1}^{n}y_i = n\bar{y}\). Therefore, \(\bar{x} (\sum_{i=1}^{n}y_i - n\bar{y}) = 0\). The same thing applies to the denominator, we have the final equation:&lt;/p&gt;

\[{\color{Blue} {\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}}}}\]</content><author><name>Chloe Nguyen</name></author><category term="supervised-learning" /><category term="regression" /><summary type="html">This post sheds light on the motivation of Simple Linear Regression and discusses the two optimization methods Least Squares VS. Gradient Descent. Formulas derivation is also included at the end of the post as a bonus.</summary></entry></feed>