<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-13T01:47:56+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chloe’s Tech Journey</title><subtitle>Summarize my learning journey</subtitle><author><name>Chloe Nguyen</name></author><entry><title type="html">MNIST Tutorial - Neural Networks approach from scratch in Python</title><link href="http://localhost:4000/deeplearning/2020/11/08/mnist-tutorial-basic-neural-network.html" rel="alternate" type="text/html" title="MNIST Tutorial - Neural Networks approach from scratch in Python" /><published>2020-11-08T14:08:00+11:00</published><updated>2020-11-08T14:08:00+11:00</updated><id>http://localhost:4000/deeplearning/2020/11/08/mnist-tutorial-basic-neural-network</id><content type="html" xml:base="http://localhost:4000/deeplearning/2020/11/08/mnist-tutorial-basic-neural-network.html">&lt;blockquote&gt;
  &lt;p&gt;A neural network = Forward path + Backward path. Simple, isn’t it? Let’s build one from scratch and classify handwritten digits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;These days, popular programming frameworks offer users convenient functions to construct a variety of neural networks using a few lines of code. But sometimes knowing how things work behind the scene can be interesting so in this tutorial, I will show you how to build a minimal neural network from scratch in Python. The objective is to use this model to classify handwritten digits from the common dataset MNIST.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mnist-dataset.png&quot; alt=&quot;MNIST Sample Images&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 1. Sample images from MNIST dataset (Image source: &lt;a href=&quot;https://www.wikiwand.com/en/MNIST_database&quot;&gt;Wikiwand&lt;/a&gt;).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;mark&gt;&lt;b&gt;Highlights&lt;/b&gt;&lt;/mark&gt;&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#overview&quot; id=&quot;markdown-toc-overview&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-by-step-implementation&quot; id=&quot;markdown-toc-step-by-step-implementation&quot;&gt;Step-by-step Implementation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#step-1-download-dataset&quot; id=&quot;markdown-toc-step-1-download-dataset&quot;&gt;Step 1: Download dataset&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-2-load-the-data&quot; id=&quot;markdown-toc-step-2-load-the-data&quot;&gt;Step 2: Load the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-3-helper-functions&quot; id=&quot;markdown-toc-step-3-helper-functions&quot;&gt;Step 3: Helper functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-4-forward-path&quot; id=&quot;markdown-toc-step-4-forward-path&quot;&gt;Step 4: Forward path&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-5-backward-path&quot; id=&quot;markdown-toc-step-5-backward-path&quot;&gt;Step 5: Backward path&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-6-putting-it-all-together&quot; id=&quot;markdown-toc-step-6-putting-it-all-together&quot;&gt;Step 6: Putting it all together&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#wrap-up-highlights&quot; id=&quot;markdown-toc-wrap-up-highlights&quot;&gt;Wrap-up Highlights&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;We will construct a minimal Neural network with one hidden layer and a 10-class softmax classifier output layer. My advice is to draft out the formulas and dimensions of all parameters beforehand which will help us debug the codes easier. The plan is to &lt;strong&gt;(1)&lt;/strong&gt; load the dataset, &lt;strong&gt;(2)&lt;/strong&gt; build the forward path, &lt;strong&gt;(3)&lt;/strong&gt; code the backward path and &lt;strong&gt;(4)&lt;/strong&gt; combine everything into a model. Along the way, we will define some helper functions.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mnist-tutorial-nn-ach.png&quot; alt=&quot;Neural Network Architecture&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 2. The minimal Neural Network has one hidden layer and a softmax classifier.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;step-by-step-implementation&quot;&gt;Step-by-step Implementation&lt;/h2&gt;

&lt;h3 id=&quot;step-1-download-dataset&quot;&gt;Step 1: Download dataset&lt;/h3&gt;
&lt;p&gt;You can download &lt;strong&gt;MNIST dataset&lt;/strong&gt; from various sources, I download mine from &lt;a href=&quot;https://www.kaggle.com/oddrationale/mnist-in-csv&quot;&gt;Kaggle&lt;/a&gt;. Simply save and unzip the package. There are two files in the folder: &lt;em&gt;mnist_train.csv&lt;/em&gt; and &lt;em&gt;mnist_test.csv&lt;/em&gt;, each contain both the &lt;strong&gt;features (X)&lt;/strong&gt; and the &lt;strong&gt;label (Y)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-2-load-the-data&quot;&gt;Step 2: Load the data&lt;/h3&gt;
&lt;p&gt;Firstly, we need to separate &lt;strong&gt;features (X)&lt;/strong&gt; and &lt;strong&gt;label (Y)&lt;/strong&gt; and perform some data preprocessing operations on them. The label is the first column followed by 784 columns of image pixels (i.e. X are 28x28 images) in both the training and testing sets. The training set has 60,000 entries while the testing set has 10,000.&lt;/p&gt;

&lt;p&gt;Because X are grayscale images, the pixel values vary greatly from 0 to 255, we can normalize them by dividing all pixels by 255. On the other hand, we need to convert the labels into one-hot vectors to classify the 10 digits (0 - 9). I will summarize the process in Fig. 3, the same operations are applied to the test set.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mnist-tutorial-nn-data.png&quot; alt=&quot;Data&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 3. demonstrates data preparation and dimensions of X and Y in MNIST training dataset.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Here is a handy code snippet to convert the labels into one-hot embeddings:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-3-helper-functions&quot;&gt;Step 3: Helper functions&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Initialize parameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the weights (W) should have dimensions of \((n^{[l]}, n^{[l-1]})\), where \(n^{[l]}\) is the number of neurons in the current layer and \(n^{[l]}\) is that of the previous one. I recommend applying &lt;a href=&quot;https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78&quot;&gt;He initialization&lt;/a&gt; to optimize the initializer. For the biases, we can initialize them to be zero vectors with the dimension of the current layer.&lt;/p&gt;

&lt;p&gt;Note: The way I set &lt;code&gt;layers = [50, 10]&lt;/code&gt; does not include the dimension of the Input layer (X), therefore, I specify two cases in the code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ReLU function simply outputs Z if Z is positive and 0 otherwise. You can make use of the &lt;code&gt;maximum()&lt;/code&gt; function in numpy library.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Softmax&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We will use Softmax activation to generate the probability for each digit in the 10-dimension vector. We can construct the Softmax activation using the formula:&lt;/p&gt;

\[A^{L}_{i} =  \frac{e^{z^{L}_{i}}}{\sum_{j=1}^{N}e^{z^{L}_{j}}}\]

&lt;p&gt;where \(A^{L}_{i}\) is the probability of each node and K is the number of nodes/classifiers of the last layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compute cost&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The cost function of Softmax for one example (for K classes) is:&lt;/p&gt;

&lt;p&gt;&lt;mark&gt;$$ - \sum_{j=1}^{K} y_j \; log (\hat{y}_j)$$&lt;/mark&gt;&lt;/p&gt;

&lt;p&gt;To calculate the loss, sum the cost of all training examples. This function serves two purposes. Firstly, by printing the cost, we can observe the trend of the model. If the cost does not decrease gradually, this means there are probably some bugs in the code. Secondly, because the cost function fluctuates across iterations, by tracking the training and testing (validating) cost, we can decide on the optimal number of epoch that minimize the cost of testing dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update parameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To keep things simple, in the tutorial, I will apply Gradient Descent to optimize the weights and biases for each layer:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Predict&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Evaluating the accuracy of the model can be as straightforward as run a forward round using the &lt;em&gt;optimal&lt;/em&gt; weights and biases and compare the results to the ground truth labels. Note that since the outputs of the model are 10-dimensional vectors, we need to transform the outcomes to its single-digit form as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-4-forward-path&quot;&gt;Step 4: Forward path&lt;/h3&gt;

&lt;p&gt;To calculate the forward path, we calculate Z and A of layer \([l]\) using the formula:&lt;/p&gt;

\[Z^{[l]} = W^{[l]} \times A^{[l-1]} + b^{[l]}\]

\[A^{[l]} = g^{[l]}(Z^{[l]})\]

&lt;p&gt;where \(g^{[l]}\) is the activation of layer \(l\) (ReLU and Softmax).&lt;/p&gt;

&lt;p&gt;We store Z and A as dictionaries to access them later for the backward path and to make prediction. Remember that our model only has one 50-unit hidden layer and a 10-class softmax layer, we define &lt;code&gt;layers = [50, 10]&lt;/code&gt;. Therefore, we set the first previous &lt;code&gt;A_prev = X&lt;/code&gt;. For the iteration, we will pass in &lt;code&gt;L = len(layers)&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;                                  &lt;span class=&quot;c1&quot;&gt;# Initiate X as A[0]
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;                   &lt;span class=&quot;c1&quot;&gt;# Update A of previous layers
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;            
            &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-5-backward-path&quot;&gt;Step 5: Backward path&lt;/h3&gt;

&lt;p&gt;To update the parameters (W and b), we need to find the derivative of the loss function with respect to each of the parameter. This can be done using the following formulas:&lt;/p&gt;

\[dZ^{[L]} = A^{[L]} - Y\]

\[dZ^{[l]} = dA^{[l]} \times g^{[l]'}(Z^{[l]})\]

\[dW^{[l]} = \frac{1}{m} dZ^{[l]} \times A^{[l-1]}.T\]

\[db^{[l]} = \frac{1}{m} sum(dZ^{[l]})\]

\[dA^{[l]} = W^{[l+1]}.T \times dZ^{[l+1]}\]

&lt;p&gt;Because &lt;code&gt;layers = [50,10]&lt;/code&gt;, &lt;code&gt;range(L)&lt;/code&gt; means to run from 0 to L-1 (i.e. layer 0 (hidden layer) and 1 (output layer)). The reversed sequence will access data from layer L-1 to 0 as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Calculate dZ for the last layer separately from others (as it uses Softmax activation)
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Derivative of Relu activation function = 0 if Z &amp;lt; 0 and = 1 otherwise
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-6-putting-it-all-together&quot;&gt;Step 6: Putting it all together&lt;/h3&gt;

&lt;p&gt;Now we have had all the components required to build the model. The objective is to train the model with training data and use the optimal weights and biases to make prediction for the testing data. To capture the optimal parameters, we choose those that minimize the test set cost. Later, we use these optimal values and run &lt;code&gt;predict&lt;/code&gt; function to calculate Test Accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Side Note:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When training the model for hundreds or thousands of iterations, it is handy to have some visualization assistance which shows a progress bar and estimates the remaining time to complete training. In this post, I imported &lt;blue&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/blue&gt; library for that purpose.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can easily experiment with other fully-connected neural network architectures by changing &lt;code&gt;layers = [ ]&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pbar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                            
        &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Z_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;min_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;min_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;W_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;b_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_description&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Cost Train = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;  -  Cost Test = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Optimal Epoch: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_epoch&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;  -  Test Accuracy = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;wrap-up-highlights&quot;&gt;Wrap-up Highlights&lt;/h2&gt;

&lt;p&gt;Voila! Now you know how to construct a minimal Neural Network to predict handwritten digits. Although our model is simplistic, the results are pretty decent with &lt;strong&gt;94% accuracy&lt;/strong&gt; on the test data. This post is a stepping stone for you to customize your model. In reality, people rarely have to code from scratch thanks to the convenience programming frameworks offer. Hopefully, this sneakpeek into how basic functions work behind the scene can be useful to you in some way.&lt;/p&gt;

&lt;p&gt;Full source code is available on my &lt;a href=&quot;&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;/ol&gt;</content><author><name>Chloe Nguyen</name></author><category term="deeplearning" /><category term="neural-networks" /><category term="deeplearning" /><category term="mnist" /><summary type="html">A neural network = Forward path + Backward path. Simple, isn’t it? Let’s build one from scratch and classify handwritten digits.</summary></entry><entry><title type="html">Simple Linear Regression: Least Squares VS. Gradient Descent</title><link href="http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression.html" rel="alternate" type="text/html" title="Simple Linear Regression: Least Squares VS. Gradient Descent" /><published>2020-11-06T00:04:00+11:00</published><updated>2020-11-06T00:04:00+11:00</updated><id>http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression</id><content type="html" xml:base="http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression.html">&lt;blockquote&gt;
  &lt;p&gt;This post sheds light on the motivation of Simple Linear Regression and discusses the two optimization methods Least Squares VS. Gradient Descent. Formulas derivation is also included at the end of the post as a bonus.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;
&lt;p&gt;It is human nature to be certain about our future and to make predictions based on existing clues. Hence, the idea of studying about the &lt;strong&gt;association&lt;/strong&gt; among subjects interests us. The first entry of the blog is dedicated to such an idea, it is about one basic concept in supervised learning: &lt;strong&gt;Simple Linear Regression&lt;/strong&gt;.&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot; id=&quot;markdown-toc-motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#least-squares-method&quot; id=&quot;markdown-toc-least-squares-method&quot;&gt;Least Squares Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-descent-method&quot; id=&quot;markdown-toc-gradient-descent-method&quot;&gt;Gradient Descent Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#model-evaluation&quot; id=&quot;markdown-toc-model-evaluation&quot;&gt;Model Evaluation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#residual-standard-error-rse&quot; id=&quot;markdown-toc-residual-standard-error-rse&quot;&gt;Residual Standard Error (RSE)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#coefficient-of-determination-r2&quot; id=&quot;markdown-toc-coefficient-of-determination-r2&quot;&gt;Coefficient of Determination (\(R^{2}\))&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#key-takeaways&quot; id=&quot;markdown-toc-key-takeaways&quot;&gt;Key Takeaways&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#limitation&quot; id=&quot;markdown-toc-limitation&quot;&gt;Limitation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bonus-least-squares---formula-derivation&quot; id=&quot;markdown-toc-bonus-least-squares---formula-derivation&quot;&gt;BONUS: Least Squares - Formula Derivation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#find-beta_0&quot; id=&quot;markdown-toc-find-beta_0&quot;&gt;Find \(\beta_0\)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#find-beta_1&quot; id=&quot;markdown-toc-find-beta_1&quot;&gt;Find \(\beta_1\)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Simple Linear Regression assumes that the independent \((x)\) and dependent \((y)\) variables have a linear relationship. Our estimation of relationship is summarized in the formula below:&lt;/p&gt;

\[\hat{y} = \beta_0 + \beta_1x\]

&lt;p&gt;\(\beta_0\) is the intercept and \(\beta_1\) is the coefficient (or slope). The name &lt;em&gt;linear regression&lt;/em&gt; suggests that the relationship resembles a straight line.&lt;/p&gt;

&lt;figure class=&quot;width_400&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slr-regplot.png&quot; alt=&quot;SLR Regplot&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 1. Visualization of a Simple Linear Regression problem&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;However, there can hardly be a perfect line that fits all the observations. Hence, errors are expected and we should find a way to minimize them. These errors are called &lt;strong&gt;Residual Sum of Squares (RSS)&lt;/strong&gt; which measures the total of &lt;em&gt;squared&lt;/em&gt; differences between our predictions and the true values (the differences are squared to prevent negative and positive values from cancelling out each other). The formula for RSS is as follows:&lt;/p&gt;

\[RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^{2}\]

&lt;p&gt;In this post, I will discuss two different approaches to find the “perfect” line by optimizing RSS. While Least Squares method provides a &lt;em&gt;one-shot&lt;/em&gt; solution for the problem, Gradient Descent &lt;em&gt;gradually adapts&lt;/em&gt; to it.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/least-squares-vs-gradient-descent.png&quot; alt=&quot;Compare Least Squares and Gradient Descent&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 2. Basic ideas of Least Squares method and Gradient Descent&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;least-squares-method&quot;&gt;Least Squares Method&lt;/h2&gt;

&lt;p&gt;The objective is to find values of \(\beta_0\) and \(\beta_1\) that minimize RSS. We know from calculus that by taking the first derivative of a function (e.g \(f(x)\)), and setting it to 0, we can solve for critical values that minimize \(f(x)\). This method works because the RSS function is convex, therefore, it has one global minimum.&lt;/p&gt;

&lt;figure class=&quot;width_400&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/3d-rss-slr.png&quot; alt=&quot;3D plot of RSS&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 3. 3D plot of Residual Sum of Squares (RSS) (Source: &lt;a href=&quot;http://faculty.marshall.usc.edu/gareth-james/ISL/&quot;&gt;Introduction to Statistical Learning&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The details of how to derive the formulas are included in the &lt;a href=&quot;## BONUS: Least Squares - Formula Derivation&quot;&gt;BONUS Section&lt;/a&gt;. To summarize, we can obtain \(\beta_0\) and \(\beta_1\) using:&lt;/p&gt;

\[\beta_0 = \bar{y} - \beta_1 \bar{x}\]

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}}\]

&lt;p&gt;By plugging in these two parameters, we can now find the perfect line to fit our data and predict future values: \(\hat{y}_i = \beta_0 + \beta_1 x_i\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using Least Squares method, we can calculate the exact values of \(\beta_0\) and \(\beta_1\) using all \(x\) and \(y\) values in &lt;strong&gt;one shot&lt;/strong&gt;. The idea is &lt;strong&gt;straightforward&lt;/strong&gt; and well-known. For Simple Linear Regression, applying this method is effective because we only deal with two variables. This method is suitable for smaller datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Least Squares method is &lt;strong&gt;limited to only a few loss functions&lt;/strong&gt; of certain shapes (e.g. RSS is a convex function). This method is expensive to implement on multivariate problems and mega datasets when &lt;strong&gt;complex matrix operations&lt;/strong&gt; involve (will be discussed later in Multiple Regression module). Therefore, instead of arriving at the bottom of the bowl (i.e. reaching the global minimum) in one go, we can go downhill gradually. That is the inspiration of Gradient Descent.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-method&quot;&gt;Gradient Descent Method&lt;/h2&gt;
&lt;p&gt;Gradient Descent is a more general concept compared to Least Squares as it can be used on various loss functions. In the application of Simple Linear Regression, we can also use Gradient Descent to optimize RSS. The idea of Gradient Descent, as its name suggests, is to update the parameters gradually until the result reaches a certain threshold or completes a desired number of iterations (i.e. &lt;em&gt;epoches&lt;/em&gt;). You can set up a &lt;em&gt;for-loop&lt;/em&gt; in Python to run Gradient Descent using the formula:&lt;/p&gt;

\[\beta_k = \beta_k - \alpha \times \frac{\partial RSS}{\partial \beta_k}  \;\; ; \;\; k =0, 1\]

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gradient Descent works on a &lt;strong&gt;diversity of loss functions&lt;/strong&gt;. Because of its versatility, Gradient Descent is a popular optimization technique used in many machine learning applications. Another benefit is that the computation step of updating the parameter is quite simple (usually the difficult part of complex models is to calculate the derivative of the loss function with respect to the parameters). Therefore, when dealing with multivariate problems such as Multiple Regression for &lt;strong&gt;large datasets&lt;/strong&gt;, Gradient Descent has an edge over Least Square method (by not having to calculate the matrix inverses - we will discuss this in later entries).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/learning-rate-gradient-descent.png&quot; alt=&quot;Learning rates in Gradient Descent&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 4. The choice of Learning rate can greatly affect Gradient Descent performance.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Choosing the hyperparameter - &lt;strong&gt;learning rate (\(\alpha\)) can be a trouble&lt;/strong&gt; in Gradient Descent. Learning rate determines the step size of each iteration. If \(\alpha\) is too large, there will be lots of deviation and it is difficult to hit the global minimum, or cannot hit the spot at all! If \(\alpha\) is too small, it will take a long time to reach the global minimum. Learning rate is a hyperparameter meaning that we need to allocate a value for it. &lt;em&gt;“Is there a perfect learning rate?”&lt;/em&gt; - you may ask, it &lt;strong&gt;depends on the problem&lt;/strong&gt;, but some popular choices are 0.1, 0.05, 0.01 or smaller values, but we don’t always use a constant learning rate to run the whole model. For instance, in deep learning applications, we usually use learning rate decay (e.g. divide the learning rate in half after every 50 iterations) to prevent the gradients from exploding or vanishing. But for now, let’s look at &lt;em&gt;Fig. 4&lt;/em&gt; to see what different choices of learning rate look like in Simple Linear Regression.&lt;/p&gt;

&lt;h2 id=&quot;model-evaluation&quot;&gt;Model Evaluation&lt;/h2&gt;

&lt;p&gt;After fitting the model, we should check whether we have done a good job or if there are any mistakes. To assess the accuracy of the model, we will discuss two related quantities: Residual Standard Error (RSE) and Coefficient of Determination (\(R^{2}\)) &lt;a class=&quot;citation&quot; href=&quot;#james2013introduction&quot;&gt;(James et al., 2013)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;residual-standard-error-rse&quot;&gt;Residual Standard Error (RSE)&lt;/h3&gt;
&lt;p&gt;RSE is the measurement for the &lt;em&gt;lack of fit&lt;/em&gt;. Although we have applied discussed techniques to achieve the “best” line, we would not be able to fit that line through all data points. This is because the relationship between \(X\) and \(Y\) are not perfectly linear. Comparing the &lt;em&gt;true&lt;/em&gt; equation \(y = \beta_0 + \beta_1x + \epsilon\) to our &lt;em&gt;estimated&lt;/em&gt; equation \(y = \beta_0 + \beta_1x\), it is clear that the error term \(\epsilon\) is not covered.&lt;/p&gt;

\[RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}}\]

&lt;p&gt;If \(y_i \approx \hat{y}_i\), which indicates that our prediction is close to the true value, RSE will be small and we can conclude that our model is a good fit for the data. On the other hand, if the model is not a good fit, \((y_i - \hat{y}_i)^{2}\) will be large and so as RSE.&lt;/p&gt;

&lt;h3 id=&quot;coefficient-of-determination-r2&quot;&gt;Coefficient of Determination (\(R^{2}\))&lt;/h3&gt;
&lt;p&gt;On the other end of the spectrum, \(R^{2}\) is the measurement of &lt;em&gt;model good fit&lt;/em&gt;. It can be interpreted as &lt;strong&gt;the amount of variations in \(Y\) that can be explained by \(X\)&lt;/strong&gt; using our model.&lt;/p&gt;

\[R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}}{\sum_{i=1}^{n}(y_i - \bar{y})^{2}}\]

&lt;p&gt;RSS is the variations left in \(Y\) after fitting our model, this is how much the model is lacking. TSS is the total variations (or the natural variations) in \(Y\) before fitting the model. Therefore, \(R^2\) shows how much variations in \(Y\) are resolved by our model. \(R^2\) score lies between 0 and 1. For example, \(R^2 = 0.75\) means that \(75\%\) of the variations in \(Y\) is explained by our model. A higher value of \(R^2\) indicates great goodness of fit and vice versa.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;h3 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h3&gt;
&lt;p&gt;In this post, we cover the core ideas of Simple Linear Regression and discuss two methods to optimize this model. The Least Squares method is straightforward and allows us to find \(\beta_0\) and \(\beta_1\) directly using two formulas. Gradient Descent can be used for a diversity of loss functions and can arrive at the solution gradually as long as we set a reasonable learning rate.&lt;/p&gt;

&lt;p&gt;After optimizing the model, we can assess its accuracy using RSE and \(R^2\), which represents the lack of fit and goodness of fit respectively. Thus, a good model should have low \(RSE\) and high \(R^2\).&lt;/p&gt;

&lt;h3 id=&quot;limitation&quot;&gt;Limitation&lt;/h3&gt;
&lt;p&gt;Though the simplicity of the model is a good point, Simple Linear Regression has many shortcomings. Today, we will briefly look at two major limitations. Firstly, Simple Linear Regression has an oversimplified assumption. In reality, many relationships are non-linear. Hence, forcing the relationship to be linear will not produce accurate predictions. Secondly, there can be many contributing factors to a problem and they can also have some interaction effects. Therefore, instead of having multiple Simple Linear Regression models, there should be an upgrade that takes into account the &lt;em&gt;synergy&lt;/em&gt; among variables. To answer that, we will consider &lt;strong&gt;Multiple Linear Regression&lt;/strong&gt; in the next entry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chloe’s End Note&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Thank you so much for checking out my first post. Please let me know if you have any ideas on improving this post. As a bonus, I have included the details on how to derive Least Squares formulas. Hope that they will be helpful to someone. Also, stay tuned for my future posts :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;bonus-least-squares---formula-derivation&quot;&gt;BONUS: Least Squares - Formula Derivation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Find values of \(\beta_0\) and \(\beta_1\) that minimize RSS:&lt;/p&gt;

\[RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^{2}\]

&lt;h3 id=&quot;find-beta_0&quot;&gt;Find \(\beta_0\)&lt;/h3&gt;
&lt;p&gt;First, find the partial derivative of RSS with respect to \(\beta_0\) treating \(x, y\) and \(\beta_1\) as constants:&lt;/p&gt;

\[\frac{\partial RSS}{\partial \beta_0} = -2 \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i)\]

&lt;p&gt;Next, we set this value to 0 and solve for \(\beta_0\):&lt;/p&gt;

\[\begin{aligned}
-2 \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i) &amp;amp; = 0 \\
\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i) &amp;amp; = 0 \\
\sum_{i=1}^{n}y_i - n\beta_0 - n\beta_1 \sum_{i=1}^{n}x_i &amp;amp; = 0 \\
\end{aligned}\]

\[\beta_0 = \frac{\sum_{i=1}^{n}y_i - \beta_1 \sum_{i=1}^{n}x_i}{n}\]

&lt;p&gt;Because:  \(\frac{\sum_{i=1}^{n}x_i}{n} = \bar{x}\)   and    \(\frac{\sum_{i=1}^{n}y_i}{n} = \bar{y}\):&lt;/p&gt;

\[{\color{Blue} {\beta_0 = \bar{y} - \beta_1 \bar{x}}}\]

&lt;h3 id=&quot;find-beta_1&quot;&gt;Find \(\beta_1\)&lt;/h3&gt;
&lt;p&gt;Similarly, we find the partial derivative of RSS with respect to \(\beta_1\) and set it to 0:&lt;/p&gt;

\[\begin{aligned}
\frac{\partial RSS}{\partial \beta_1} = -2 \sum_{i=1}^{n}x_i(y_i - \beta_0 - \beta_1x_i) &amp;amp; = 0 \\
\sum_{i=1}^{n}x_iy_i - \beta_0 \sum_{i=1}^{n}x_i - \beta_1 \sum_{i=1}^{n}x_i^{2} &amp;amp; = 0
\end{aligned}\]

&lt;p&gt;Plug in \(\beta_0 = \bar{y} - \beta_1 \bar{x}\):&lt;/p&gt;

\[\begin{aligned}
\sum_{i=1}^{n}x_iy_i - (\bar{y} - \beta_1 \bar{x})\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^{2} &amp;amp; = 0 \\
\sum_{i=1}^{n}x_iy_i - \bar{y}\sum_{i=1}^{n}x_i + \beta_1 \bar{x}\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^{2} &amp;amp; = 0 \\
\sum_{i=1}^{n}x_i(y_i - \bar{y}) - \beta_1 \sum_{i=1}^{n}x_i (x_i - \bar{x}) &amp;amp; = 0 \\
\end{aligned}\]

&lt;p&gt;Solve for \(\beta_1\):&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x})}\]

&lt;p&gt;We can conclude here and use this equation to find \(\beta_1\), but the more widely used variation in books and lecture notes is:&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}} = \frac{S_{XY}}{S_{XX}}\]

&lt;p&gt;Many people find the equation above easier to remember as its numerator \((S_{XY})\) and denominator \((S_{XX})\) are the outputs of some software functions. With a few algebra adjustments, we can transform our original equation into its well-known form:&lt;/p&gt;

\[\begin{aligned}
\beta_1 
&amp;amp;= \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x})} \\
&amp;amp;= \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y}) - \sum_{i=1}^{n} \bar{x} (y_i - \bar{y}) + \sum_{i=1}^{n} \bar{x} (y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x}) - \sum_{i=1}^{n}\bar{x}(x_i - \bar{x}) + \sum_{i=1}^{n}\bar{x}(x_i - \bar{x})} \\
&amp;amp;= \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y}) + \bar{x} (\sum_{i=1}^{n}y_i - n\bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2} + \bar{x} (\sum_{i=1}^{n}x_i - n\bar{x})}
\end{aligned}\]

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In the last term of the equation in the numerator, (1) \(\bar{x}\) is a number, so it can come out of the sum and (2) \(\sum_{i=1}^{n}y_i = n\bar{y}\). Therefore, \(\bar{x} (\sum_{i=1}^{n}y_i - n\bar{y}) = 0\). The same thing applies to the denominator, we have the final equation:&lt;/p&gt;

\[{\color{Blue} {\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}}}}\]

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;james2013introduction&quot;&gt;James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2013). &lt;i&gt;An introduction to statistical learning&lt;/i&gt; (Vol. 112). Springer.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Chloe Nguyen</name></author><category term="supervised-learning" /><category term="regression" /><category term="least-squares" /><category term="gradient-descent" /><summary type="html">This post sheds light on the motivation of Simple Linear Regression and discusses the two optimization methods Least Squares VS. Gradient Descent. Formulas derivation is also included at the end of the post as a bonus.</summary></entry></feed>