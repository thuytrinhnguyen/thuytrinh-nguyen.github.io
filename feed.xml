<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-03T00:19:00+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chloe’s Tech Journey</title><subtitle>Summarize my learning journey</subtitle><author><name>Chloe Nguyen</name></author><entry><title type="html">Entropy - Information Gain Theory</title><link href="http://localhost:4000/2020/11/23/entropy.html" rel="alternate" type="text/html" title="Entropy - Information Gain Theory" /><published>2020-11-23T11:46:00+11:00</published><updated>2020-11-23T11:46:00+11:00</updated><id>http://localhost:4000/2020/11/23/entropy</id><content type="html" xml:base="http://localhost:4000/2020/11/23/entropy.html">&lt;blockquote&gt;
  &lt;p&gt;Entropy is a basic concept that you might have come across while studying about Decision Tree or machine learning cross-entropy loss function. This post will address the two interpretations of entropy to help you understand the concept better.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;mark&gt;&lt;b&gt;Highlights&lt;/b&gt;&lt;/mark&gt;&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#measure-of-uncertainty&quot; id=&quot;markdown-toc-measure-of-uncertainty&quot;&gt;Measure of uncertainty&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#two-state-weather-with-equal-likelihood&quot; id=&quot;markdown-toc-two-state-weather-with-equal-likelihood&quot;&gt;Two-state weather with equal likelihood&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eight-state-weather-with-equal-likelihood&quot; id=&quot;markdown-toc-eight-state-weather-with-equal-likelihood&quot;&gt;Eight-state weather with equal likelihood&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#events-with-different-probabilities&quot; id=&quot;markdown-toc-events-with-different-probabilities&quot;&gt;Events with different probabilities&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#measure-of-useful-information-information-gain&quot; id=&quot;markdown-toc-measure-of-useful-information-information-gain&quot;&gt;Measure of useful information (Information gain)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#relationship-between-entropy-and-information&quot; id=&quot;markdown-toc-relationship-between-entropy-and-information&quot;&gt;Relationship between Entropy and Information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#why-information-gain-matters&quot; id=&quot;markdown-toc-why-information-gain-matters&quot;&gt;Why information gain matters&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#applications&quot; id=&quot;markdown-toc-applications&quot;&gt;Applications&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#key-takeaways&quot; id=&quot;markdown-toc-key-takeaways&quot;&gt;Key Takeaways&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Entropy is a concept of the information theory introduced by Shannon Claude. Today I will discuss two interpretations of this concept, entropy as (1) a measure of uncertainty and as (2) a measure of useful information.&lt;/p&gt;

&lt;p&gt;The formula for entropy is:&lt;/p&gt;

\[H(p) = -\sum_{i} p_i \times log(p_i)\]

&lt;p&gt;where \(i\) is the total number of possible events and \(p_i\) is the probability of each event.&lt;/p&gt;

&lt;p&gt;Since \(log(\frac{1}{p_i}) = - log(p_i)\), another way to phrase this: \(H(p) = \sum_{i} p_i \times log(\frac{1}{p_i})\).&lt;/p&gt;

&lt;h2 id=&quot;measure-of-uncertainty&quot;&gt;Measure of uncertainty&lt;/h2&gt;

&lt;p&gt;There are many explanations around entropy, among which entropy as a measure of uncertainty is the most popular idea.&lt;/p&gt;

&lt;p&gt;Think of entropy as &lt;strong&gt;How many yes-no question needed on average to reach a decision?&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;two-state-weather-with-equal-likelihood&quot;&gt;Two-state weather with equal likelihood&lt;/h3&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/entropy-2-50.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 1. Two-state weather with 50% probability each&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For example, if there are only 2 states of the weather &lt;em&gt;Sunny&lt;/em&gt; and &lt;em&gt;Rainy&lt;/em&gt;, you can just ask one question &lt;em&gt;“Is it sunny tomorrow?”&lt;/em&gt;, whether the answer is yes or no, you will know the weather tomorrow in one question. The entropy of this scenario is:&lt;/p&gt;

\[H_{2\_states\_equal} = - 0.5 \times log(0.5) - 0.5 \times log(0.5) = 1\]

&lt;h3 id=&quot;eight-state-weather-with-equal-likelihood&quot;&gt;Eight-state weather with equal likelihood&lt;/h3&gt;

&lt;p&gt;What if there are 8 equally-likely states? You will probably need 3 questions to come to the final answer demonstrated in Fig. 2.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Question 1.&lt;/strong&gt; Is the weather in [the upper 4 states]?&lt;/p&gt;

    &lt;p&gt;Yes \(\rightarrow\) Eliminate 4 lower states, 4 upper states remaining.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Question 2.&lt;/strong&gt; Is the weather in [the right hand side 2 states]?&lt;/p&gt;

    &lt;p&gt;No \(\rightarrow\) Eliminate 2 right hand side states, 2 states (&lt;em&gt;Sunny&lt;/em&gt; &amp;amp; &lt;em&gt;Partially Cloudy&lt;/em&gt;) remaining.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Question 3.&lt;/strong&gt; Is the weather &lt;em&gt;Sunny&lt;/em&gt;?&lt;/p&gt;

    &lt;p&gt;Yes \(\rightarrow\) Eliminate &lt;em&gt;Partially Cloudy&lt;/em&gt;, the final answer is Sunny.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As expected, the entropy is this scenario is:  \(H_{8\_states\_equal} = - 8 \times 0.125 \times log(0.125) = 3\)&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/entropy-8-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 2. Eight-state weather with 12.5% probability each&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;From these examples, we reach two conclusions:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;mark&gt;(1)&lt;/mark&gt; Entropy is the number of yes/no question &lt;em&gt;on average&lt;/em&gt; to classify the data.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;mark&gt;(2)&lt;/mark&gt; The higher number of classifiers, the more random the data, hence, the higher the entropy.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But, is the number of classifiers the only indicator of uncertainty in a dataset? So far we have only considered cases of events with equal probabilities, let’s have some examples of classes with different likelihood to compare their entropy.&lt;/p&gt;

&lt;h3 id=&quot;events-with-different-probabilities&quot;&gt;Events with different probabilities&lt;/h3&gt;

&lt;p&gt;In a tropical country, the weather is mostly sunny (75% of the time) and rainy sometimes (25% of the time).&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/entropy-2-75.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 3. Two-state weather with 75% probability sunny and 25% rainy&lt;/figcaption&gt;
&lt;/figure&gt;

\[H_{2\_states\_skewed} = - 0.75 \times log(0.75) - 0.25 \times log(0.25)  = 0.81\]

&lt;p&gt;This time, when calculating entropy (the number of questions), we also need to consider the weights of each event. The entropy in this case is \(0.81\), which is less than when Sunny and Rainy weather are equally likely. To confirm whether it is true that we can ask fewer questions when the probabilities of events are not equal, let’s take another example of a 8-state weather of another tropical country.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/entropy-8-35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 4. Eight-state weather with mixed probabilities&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Let’s calculate the entropy of this eight-state weather with mixed probabilities:&lt;/p&gt;

\[\begin{aligned}
H_{8\_states\_skewed\_35} 
&amp;amp;= - 2 \times 0.35 \times log(0.35) - 2 \times 0.1 \times log(0.1) \\
&amp;amp; \; \; \; - 2 \times 0.04 \times log(0.04) - 2 \times 0.01 \times log(0.01) \\
&amp;amp;= 2.23 
\end{aligned}\]

&lt;p&gt;What if the weather likelihood is even more skewed? Let’s calculate the entropy for this distribution: 45% - 25% - 10% - 10% - 4% - 4% - 1% - 1%.&lt;/p&gt;

\[\begin{aligned}
H_{8\_states\_skewed\_45} 
&amp;amp;= - 0.45 \times log(0.45) - 0.25 \times log(0.25) - 2 \times 0.1 \times log(0.1) \\
&amp;amp;\; \; \; - 2 \times 0.04 \times log(0.04) - 2 \times 0.01 \times log(0.01) \\
&amp;amp;= 2.19
\end{aligned}\]

&lt;p&gt;Comparison: \(H_{8\_states\_equal} &amp;gt; H_{8\_states\_skewed\_35} &amp;gt; H_{8\_states\_skewed\_45}\)&lt;/p&gt;

&lt;p&gt;Therefore, we reach the third conclusion:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;mark&gt;(3)&lt;/mark&gt; The more skewed the likelihood of events, the less uncertainty of the data, hence, the lower the entropy.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By testing the entropy against different situations, we know that conclusion (3) is an accurate statement. However, the intuition of entropy has not been channelled fully. We can look at a related term that can enhance our knowledge of entropy.&lt;/p&gt;

&lt;h2 id=&quot;measure-of-useful-information-information-gain&quot;&gt;Measure of useful information (Information gain)&lt;/h2&gt;

&lt;p&gt;A related term with entropy is information. While entropy is the &lt;strong&gt;uncertainty before&lt;/strong&gt; decision-making, information is the &lt;strong&gt;additional knowledge after&lt;/strong&gt; decision-making.&lt;/p&gt;

&lt;h3 id=&quot;relationship-between-entropy-and-information&quot;&gt;Relationship between Entropy and Information&lt;/h3&gt;

&lt;p&gt;Take an extreme example of weather in a desert which is sunny all year round \((p_{Sunny} = 1)\). What is the entropy in this case?&lt;/p&gt;

\[H_{sunny\_100} = - log(1) - 0 = 0\]

&lt;p&gt;Comparison: \(H_{2\_states\_equal} &amp;gt; H_{2\_states\_skewed} &amp;gt; H_{sunny\_100}\)&lt;/p&gt;

&lt;p&gt;When the weather is always sunny, we are 100% certain \(H_{sunny\_100} = 0\). However, since the weather is always predictable, there is no value in having weather forecast or asking any yes/no questions. Hence, the information gain is zero.&lt;/p&gt;

&lt;p&gt;When it is 50% sunny and 50% rainy, by telling that the weather will be rainy tomorrow, our uncertainty reduces by half. Hence, the information we get from knowing the weather will be rainy is: \(log(\frac{1}{0.5})=\) 2 bits of information. Similarly, if the weather station forecasts that tomorrow will be sunny, the amount of information we get is also 2 bits as the two states are equally likely to happen. Taking into account the likelihood of both events, we have the &lt;em&gt;average&lt;/em&gt; information gain in this scenario: \(0.5 \times 2 + 0.5 \times 2 = 1\), this is our entropy (\(H_{2\_states\_equal} = 1\))! Therefore, the two interpretations are both accurate, we can look at entropy from different perspectives.&lt;/p&gt;

&lt;p&gt;Think about it for a second, you should see why it totally makes sense. If the case is predictable (i.e. there is no randomness), there is no new information in telling us what we are already certain of. On the contrary, if the case has many states of different probabilities, we are not sure about the outcome, hence, there is some information gain in telling us what will happen. This takes us to our fourth conclusion:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;mark&gt;(4)&lt;/mark&gt; The higher the randomness (higher entropy), the higher the information gain.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But why do we need to know about information if it is just another way to intuitively think about entropy?&lt;/p&gt;

&lt;h3 id=&quot;why-information-gain-matters&quot;&gt;Why information gain matters&lt;/h3&gt;

&lt;p&gt;We concern about information gain because we wish to measure the effectiveness of communication: &lt;strong&gt;How to send useful information using the least number of bits?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Remember that we have calculated \(H_{2\_state\_equal} = 1\), this is 1 bit of &lt;strong&gt;useful information&lt;/strong&gt;. No matter how we convey the message, only 1 bit is useful in this case (e.g. sending 0-1 signal with 1 being sunny weather and 0 otherwise). In this example, 1 bit is the optimal number of bit we should send to have the most effective communication.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/entropy-8-3b.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Eight-state weather with 3-bit assignment&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For a 8-state weather with skewed likelihood, \(H_{8\_states\_skewed\_35} = 2.23\). This means no matter how we deliver the message, on average only 2.23 useful bits are received.&lt;/p&gt;

&lt;p&gt;If we use 3 bits to denote each state of the weather, the number of bits we send out on average is:&lt;/p&gt;

\[H= 2 \times 0.35 \times 3 + 2 \times 0.1 \times 3 + 2 \times 0.04 \times 3 + 2 \times 0.01 \times 3 = 3\]

&lt;p&gt;The result is suboptimal as we are \(0.77\) bit over the optimal value. We can try adjusting the number of bits assigned to each event and look for a combination whose total is the closet to number of useful bits.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/entropy-8-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Eight-state weather with updated bit assignments&lt;/figcaption&gt;
&lt;/figure&gt;

\[H = 2\times 0.35 \times 2 + 2 \times 0.1 \times 3 + 2 \times 0.04 \times 4 + 2 \times 0.01 \times 5 = 2.42\]

&lt;p&gt;We are becoming more effective by assigning fewer bits on the low-value events and &lt;strong&gt;more bits on the rare events&lt;/strong&gt;, therefore, we will not waste lots of bits to events that happen most of the time as expected.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;h3 id=&quot;applications&quot;&gt;Applications&lt;/h3&gt;

&lt;p&gt;Entropy is used as a splitting criteria for Decision Tree. Most commonly, people use Decision Tree for classification problems whose ultimate goal is to have pure sets in the end. Using entropy, we can measure the impurity in the set, split the data so that after each step, the data is more defined until the set has a zero entropy.&lt;/p&gt;

&lt;p&gt;Another widely used term related to entropy is cross-entropy. Instead of measuring the uncertainty within one distribution like entropy, cross-entropy compares two probability distributions. It is a popular loss function in Machine learning to compare model prediction and ground truth values.&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h3&gt;

&lt;p&gt;Understanding entropy gives you the intuitive thinking in various problems. The key takeaways from this post are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Entropy measures uncertainty:&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Entropy is the average number of yes/no questions on you ask to classify the data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The more skewed the probabilities of events are, the higher uncertainty, hence, higher entropy.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Entropy measures information gain:&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;When the event is predictable and be more likely to behave in one way, there is little information gain.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Therefore, the higher entropy (uncertainty), the higher information gain.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope that this post on entropy has been helpful. Happy learning!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chloe’s End Note&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;It takes me a full week to fully understand this seemingly straightforward concept. But it is such a relief to finally get this concept right so I can move on to Decision Tree smoothly. Also, I hope that the illustrations are joyful to look at. I had a good time drawing them :)&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Chloe Nguyen</name></author><category term="decision-tree" /><category term="information-theory" /><category term="entropy" /><summary type="html">Entropy is a basic concept that you might have come across while studying about Decision Tree or machine learning cross-entropy loss function. This post will address the two interpretations of entropy to help you understand the concept better.</summary></entry><entry><title type="html">Exploratory Data Analysis in Python</title><link href="http://localhost:4000/data-analysis/2020/11/16/exploratory-data-analysis.html" rel="alternate" type="text/html" title="Exploratory Data Analysis in Python" /><published>2020-11-16T14:08:00+11:00</published><updated>2020-11-16T14:08:00+11:00</updated><id>http://localhost:4000/data-analysis/2020/11/16/exploratory-data-analysis</id><content type="html" xml:base="http://localhost:4000/data-analysis/2020/11/16/exploratory-data-analysis.html">&lt;blockquote&gt;
  &lt;p&gt;This post will show you how to perform standard Exploratory Data Analysis using Pandas operations and visualization in Matplotlib and Seaborn libraries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Once received a dataset, is it a good idea to jump straight in and conduct some complicated statistical tests? What if you are given a massive dataset of millions records, how do you approach it? Just like meeting a stranger at your friend dinner party, we need to have a few conversations to understand each other’s background, hobbies, etc. before becoming acquaintances or even close friends. This post will show you how to approach a dataset and get to know it well. Hopefully, after reading this post you can become friends with the data you have on hand.&lt;/p&gt;

&lt;p&gt;&lt;mark&gt;&lt;b&gt;Highlights&lt;/b&gt;&lt;/mark&gt;&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot; id=&quot;markdown-toc-motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dataset&quot; id=&quot;markdown-toc-dataset&quot;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#checking-data-type&quot; id=&quot;markdown-toc-checking-data-type&quot;&gt;Checking Data Type&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#descriptive-statistics&quot; id=&quot;markdown-toc-descriptive-statistics&quot;&gt;Descriptive Statistics&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#numerical-data&quot; id=&quot;markdown-toc-numerical-data&quot;&gt;Numerical Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#categorical-data&quot; id=&quot;markdown-toc-categorical-data&quot;&gt;Categorical Data&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#missing-data&quot; id=&quot;markdown-toc-missing-data&quot;&gt;Missing data&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#replace-by-mean&quot; id=&quot;markdown-toc-replace-by-mean&quot;&gt;Replace by mean&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#replace-by-mode&quot; id=&quot;markdown-toc-replace-by-mode&quot;&gt;Replace by mode&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#do-nothing&quot; id=&quot;markdown-toc-do-nothing&quot;&gt;Do nothing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#standardization&quot; id=&quot;markdown-toc-standardization&quot;&gt;Standardization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#numerical-data-1&quot; id=&quot;markdown-toc-numerical-data-1&quot;&gt;Numerical Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#categorical-data-1&quot; id=&quot;markdown-toc-categorical-data-1&quot;&gt;Categorical Data&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Exploratory data analysis (EDA) is the fundamental step before conducting any in-depth analysis. The purpose of (EDA) is to extract the characteristics and assess the potential of each field in the dataset.&lt;/p&gt;

&lt;p&gt;Firstly, we can perform descriptive statistics to gain initial understanding of the data. Then we handle missing values to, hopefully, improve completeness and preserve information. Lastly, as the data may come in different formats, we should standardize values to aid further analysis.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;To demonstrate the steps of EDA, we will be using a sample dataset on fruit prices and other data such as the grades given by the seller, the stock quantities and the review scores given by customers. A sample preview of the data is shown below:&lt;/p&gt;

&lt;figure class=&quot;width_300&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-data.png&quot; alt=&quot;Sample Dataset&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;checking-data-type&quot;&gt;Checking Data Type&lt;/h2&gt;

&lt;p&gt;It is a good practice to check the data type of all columns before proceeding to further exploration. The method &lt;code&gt;df.dtypes&lt;/code&gt; returns a list of column name and data types. For example, in your sample dataset, &lt;strong&gt;Type&lt;/strong&gt; and &lt;strong&gt;Grade&lt;/strong&gt; have &lt;code&gt;object&lt;/code&gt; data type meaning these columns contain string values, while others have type &lt;code&gt;float64&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can change the data type of any columns using &lt;code&gt;df['column_name'].astype()&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;descriptive-statistics&quot;&gt;Descriptive Statistics&lt;/h2&gt;

&lt;p&gt;Next, we perform descriptive statistics to understand key attributes of each field (e.g. the minimum, maximum, most frequent values and the distribution of the data). Let’s review some useful functions for numerical and categorical data.&lt;/p&gt;

&lt;h3 id=&quot;numerical-data&quot;&gt;Numerical Data&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;General description&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The method &lt;code&gt;df.describe()&lt;/code&gt; in &lt;strong&gt;Pandas&lt;/strong&gt; will present a summary of some standard metrics on numerical columns dropping any NaN values. The output table gives us a rough idea of the data critical values and its distribution.&lt;/p&gt;

&lt;figure class=&quot;width_250&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-code-des.png&quot; alt=&quot;df.describe output&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Correlation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Examining the potential correlation among columns is a also good idea. We can achieve that using the method &lt;code&gt;df.corr()&lt;/code&gt;, which measures the Pearson correlation values of numerical columns. The output in our example suggests a strong relationship between the prices and customer review scores (\(\rho \approx 0.836\)).&lt;/p&gt;

&lt;figure class=&quot;width_250&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-corr.png&quot; alt=&quot;df.corr output&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Visualizing relationships&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To gain a better understanding of the data, we can observe the trend of numerical variables using a &lt;em&gt;scatterplot&lt;/em&gt;. A &lt;em&gt;regplot&lt;/em&gt; (regression plot) does the same thing but with a regression line to check if the relationship resemble a linear association. Note that these plots only picture an initial impression, we should conduct statistical tests to come to the final conclusion. We will be using &lt;strong&gt;Seaborn&lt;/strong&gt; library to plot the data. In our example, we see that the price and review of the fruits have a positive relationship. As the review scores increase, so do the prices. The &lt;em&gt;Regplot&lt;/em&gt; suggests that they have a linear relationship.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Scatter Plot&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Price'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review VS Price Scatterplot'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;figure class=&quot;width_300&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-scatter.png&quot; alt=&quot;Scatterplot&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Regression Plot&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Price'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review VS Price Regression Plot'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;width_300&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-regplot.png&quot; alt=&quot;Regplot&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;categorical-data&quot;&gt;Categorical Data&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;General description&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The method &lt;code&gt;describe()&lt;/code&gt; can also be used for categorical data if you pass in an &lt;code&gt;include&lt;/code&gt; argument. The result is a summary table for both categorical and numerical data. For example, in the &lt;code&gt;Type&lt;/code&gt; column, there are 14 values of 11 unique values which indicates the existence of various wordings for the same object.&lt;/p&gt;

&lt;figure class=&quot;width_300&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-cat-describe.png&quot; alt=&quot;df.describe(include='all')&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Unique value counts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To count the frequency of each unique value in a categorical column, we use &lt;code&gt;value_counts&lt;/code&gt;. Column &lt;code&gt;Type&lt;/code&gt; has many spelling errors which we can improve with some standardization methods in a leter section. On the other hand, column &lt;code&gt;Grade&lt;/code&gt; has only 3 values (A, B, C) among which B is the most frequent one with 5 occurences.&lt;/p&gt;

&lt;figure class=&quot;width_400&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-value-counts.png&quot; alt=&quot;Value Counts&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;missing-data&quot;&gt;Missing data&lt;/h2&gt;

&lt;p&gt;Missing data is expected in any real life datasets. To count the number of missing values in each column, call the method:  &lt;code&gt;df.isna().sum()&lt;/code&gt;. There are three popular techniques to deal with missing values: (1) Replace by mean, (2) replace by mode and (3) do nothing.&lt;/p&gt;

&lt;h3 id=&quot;replace-by-mean&quot;&gt;Replace by mean&lt;/h3&gt;

&lt;p&gt;To replace NaN values in numerical columns, we use the method &lt;code&gt;fillna()&lt;/code&gt;. In this case, I want to compare the 2 columns of Price (with and without NaN values), therefore, I assign the filled values to the column &lt;strong&gt;‘Price Without NAs’&lt;/strong&gt;. If you wish to make changes to the original column, add the argument &lt;code&gt;inplace=True&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df['Price Without NAs'] = df['Price'].fillna(df['Price'].mean())&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;width_200&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-price-full.png&quot; alt=&quot;Price Full&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;replace-by-mode&quot;&gt;Replace by mode&lt;/h3&gt;

&lt;p&gt;Similarly, we can replace NaN values in categorical columns using &lt;code&gt;fillna()&lt;/code&gt;. To access the mode, use &lt;code&gt;mode()[0]&lt;/code&gt; which will return the most common value in that column.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;df['Grade Without NAs'] = df['Grade'].fillna(df['Grade'].mode()[0])&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;width_200&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-grade-full.png&quot; alt=&quot;Grade Full&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;do-nothing&quot;&gt;Do nothing&lt;/h3&gt;

&lt;p&gt;Ideally, replacing missing values with our “best guess” to help preserve the meaning of the data. In some cases, however, leaving missing values as they are, is a more reasonable option. Let me explain. Replacing by mean or by mode has a “neutralizing” effect on the data, hence, it can alter some characteristics of the dataset if the missing values occupy a large portion of the data.&lt;/p&gt;

&lt;h2 id=&quot;standardization&quot;&gt;Standardization&lt;/h2&gt;

&lt;p&gt;Standardization makes comparing data easier and helps us see some hidden characteristics of the data. There are various techniques to standardize both numerical and categorical data.&lt;/p&gt;

&lt;h3 id=&quot;numerical-data-1&quot;&gt;Numerical Data&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Common methods to standardize values&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Simple standardization is to divide all values by the &lt;strong&gt;maximum&lt;/strong&gt;, which results in data value 0-1.&lt;/p&gt;

&lt;p&gt;MinMax standardization applies \(\frac{x - min}{max - min}\).&lt;/p&gt;

&lt;p&gt;Z-score standardization is more popular in statistics, \(z = \frac{x - \mu}{\sigma}\).&lt;/p&gt;

&lt;p&gt;The results of 3 methods are different and should be chosen depending on the domains and applications of the project.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;review_min&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;review_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;review_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;review_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review Simple Standardize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review_max&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review Minmax Standardize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review_max&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review Zscore Standardize'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Review'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review_std&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-num-std.png&quot; alt=&quot;Review Standardized&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Binning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Binning is to group data points into intervals. In other words, we can convert numerical data to categorical groups (e.g. age groups, budget levels, …).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To decide the boundaries or intervals, we should first observe the trends using a histogram. We can use methods &lt;code&gt;displot()&lt;/code&gt;, &lt;code&gt;displot()&lt;/code&gt; or &lt;code&gt;catplot(..., kind='hist',...)&lt;/code&gt; from &lt;strong&gt;Seaborn&lt;/strong&gt; to generate elegant looking histograms. Notice that you can play around with the number of &lt;code&gt;bins&lt;/code&gt; to check how many intervals work best on your data. Fig. 1. illustrates 2 histograms with 10 bins (on the left) and 5 bins (on the right).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Quantity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# or bins=5
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Quantity Histogram'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-multi-hist.png&quot; alt=&quot;Histogram&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 1. Histogram comparison between different bin sizes&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pandas.cut&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you have selected the groups to sort our data, use the method &lt;code&gt;pd.cut&lt;/code&gt; to section the data into categories. You can do it manually (i.e. by specifying the boundaries of each interval) or automatically (i.e. by dividing the sections equally). I will give an example of categorizing &lt;strong&gt;Quantity&lt;/strong&gt; values into 3 equal-interval bins which indicates levels of inventory.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;quantity_bin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Quantity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Quantity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quantity_group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Low Stock'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Medium Stock'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'High Stock'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Quantity Binned'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Quantity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantity_bin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantity_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;include_lowest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;width_200&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-binning.png&quot; alt=&quot;Binning&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;categorical-data-1&quot;&gt;Categorical Data&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Standardizing wording variations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;From previous part, we notice that there are spelling mistakes and wording variations that we can improve. The method &lt;code&gt;df.replace()&lt;/code&gt; is useful for this purpose. In our example, I standardize different types of fruit into: Apple, Orange and Lemon. I use &lt;code&gt;dict.fromkeys()&lt;/code&gt; to replace multiple values at once. To replace a single value and save the changes to the original column, use &lt;code&gt;df.replace('value to replace', 'replacing value', inplace=True).&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Type'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromkeys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'apples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Apps'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Appel'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Apple'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromkeys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'orange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'navel'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Oranges'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Orange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromkeys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pink lemons'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'lemon'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'lemmon'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Lemon'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;figure class=&quot;width_200&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-type-standard.png&quot; alt=&quot;Categorical Wording&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Groupby&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What if we want to compare the price of Apple - Grade A and Apple - Grade C (i.e. difference among Grades) or Apple - Grade A and Orange - Grade A (i.e. difference among Fruits)? We can use the method &lt;code&gt;groupby&lt;/code&gt; to categorize data. In our example, I build a table containing the average prices of fruits of each grades - think of it as a double filter!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_subset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Grade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Price Without NAs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_grp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Grade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;width_300&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-groupby.png&quot; alt=&quot;Groupby&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Pivot Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A more effective way to display the grouped data we just created is to use &lt;strong&gt;Pivot Table&lt;/strong&gt;. I run &lt;code&gt;pivot()&lt;/code&gt; method on our grouped data &lt;code&gt;df_grp&lt;/code&gt; and assign which fields to be the column and row of our pivot table. Looking much cleaner now!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_pivot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_grp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Grade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Type Standardized'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;width_300&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/eda-pivot.png&quot; alt=&quot;Pivot&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this post, I have shown you some basic techniques in Exploratory Data Analysis - to gain understanding of the data. We explore several different aspects of data preprocessing (e.g. missing values, standardization, binning, grouping), data visualization (e.g. scatterplot, histogram) and their applications on numerical and categorical data. I hope that you have learned something new today. Please leave a comment if anything in the post needs correction.&lt;/p&gt;

&lt;p&gt;Thank you very much and stay tuned for more meaningful content every week!&lt;/p&gt;</content><author><name>Chloe Nguyen</name></author><category term="data-analysis" /><category term="data-analysis" /><category term="visualization" /><summary type="html">This post will show you how to perform standard Exploratory Data Analysis using Pandas operations and visualization in Matplotlib and Seaborn libraries.</summary></entry><entry><title type="html">MNIST Tutorial - Neural Networks approach from scratch in Python</title><link href="http://localhost:4000/deeplearning/2020/11/11/mnist-tutorial-basic-neural-network.html" rel="alternate" type="text/html" title="MNIST Tutorial - Neural Networks approach from scratch in Python" /><published>2020-11-11T14:08:00+11:00</published><updated>2020-11-11T14:08:00+11:00</updated><id>http://localhost:4000/deeplearning/2020/11/11/mnist-tutorial-basic-neural-network</id><content type="html" xml:base="http://localhost:4000/deeplearning/2020/11/11/mnist-tutorial-basic-neural-network.html">&lt;blockquote&gt;
  &lt;p&gt;A neural network = Forward path + Backward path. Simple, isn’t it? Let’s build one from scratch and classify handwritten digits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;These days, popular programming frameworks offer users convenient functions to construct a variety of neural networks using a few lines of code. But sometimes knowing how things work behind the scene can be interesting so in this tutorial, I will show you how to build a minimal neural network from scratch in Python. The objective is to use this model to classify handwritten digits from the common dataset MNIST.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mnist-dataset.png&quot; alt=&quot;MNIST Sample Images&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 1. Sample images from MNIST dataset (Image source: &lt;a href=&quot;https://www.wikiwand.com/en/MNIST_database&quot;&gt;Wikiwand&lt;/a&gt;).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;mark&gt;&lt;b&gt;Highlights&lt;/b&gt;&lt;/mark&gt;&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#overview&quot; id=&quot;markdown-toc-overview&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-by-step-implementation&quot; id=&quot;markdown-toc-step-by-step-implementation&quot;&gt;Step-by-step Implementation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#step-1-download-dataset&quot; id=&quot;markdown-toc-step-1-download-dataset&quot;&gt;Step 1: Download dataset&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-2-load-the-data&quot; id=&quot;markdown-toc-step-2-load-the-data&quot;&gt;Step 2: Load the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-3-helper-functions&quot; id=&quot;markdown-toc-step-3-helper-functions&quot;&gt;Step 3: Helper functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-4-forward-path&quot; id=&quot;markdown-toc-step-4-forward-path&quot;&gt;Step 4: Forward path&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-5-backward-path&quot; id=&quot;markdown-toc-step-5-backward-path&quot;&gt;Step 5: Backward path&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#step-6-putting-it-all-together&quot; id=&quot;markdown-toc-step-6-putting-it-all-together&quot;&gt;Step 6: Putting it all together&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#wrap-up-highlights&quot; id=&quot;markdown-toc-wrap-up-highlights&quot;&gt;Wrap-up Highlights&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;We will construct a minimal Neural network with one hidden layer and a 10-class softmax classifier output layer. My advice is to draft out the formulas and dimensions of all parameters beforehand which will help us debug the codes easier. The plan is to &lt;strong&gt;(1)&lt;/strong&gt; load the dataset, &lt;strong&gt;(2)&lt;/strong&gt; build the forward path, &lt;strong&gt;(3)&lt;/strong&gt; code the backward path and &lt;strong&gt;(4)&lt;/strong&gt; combine everything into a model. Along the way, we will define some helper functions.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mnist-tutorial-nn-ach.png&quot; alt=&quot;Neural Network Architecture&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 2. The minimal Neural Network has one hidden layer and a softmax classifier.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;step-by-step-implementation&quot;&gt;Step-by-step Implementation&lt;/h2&gt;

&lt;h3 id=&quot;step-1-download-dataset&quot;&gt;Step 1: Download dataset&lt;/h3&gt;
&lt;p&gt;You can download &lt;strong&gt;MNIST dataset&lt;/strong&gt; from various sources, I download mine from &lt;a href=&quot;https://www.kaggle.com/oddrationale/mnist-in-csv&quot;&gt;Kaggle&lt;/a&gt;. Simply save and unzip the package. There are two files in the folder: &lt;em&gt;mnist_train.csv&lt;/em&gt; and &lt;em&gt;mnist_test.csv&lt;/em&gt;, each contain both the &lt;strong&gt;features (X)&lt;/strong&gt; and the &lt;strong&gt;label (Y)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-2-load-the-data&quot;&gt;Step 2: Load the data&lt;/h3&gt;
&lt;p&gt;Firstly, we need to separate &lt;strong&gt;features (X)&lt;/strong&gt; and &lt;strong&gt;label (Y)&lt;/strong&gt; and perform some data preprocessing operations on them. The label is the first column followed by 784 columns of image pixels (i.e. X are 28x28 images) in both the training and testing sets. The training set has 60,000 entries while the testing set has 10,000.&lt;/p&gt;

&lt;p&gt;Because X are grayscale images, the pixel values vary greatly from 0 to 255, we can normalize them by dividing all pixels by 255. On the other hand, we need to convert the labels into one-hot vectors to classify the 10 digits (0 - 9). I will summarize the process in Fig. 3, the same operations are applied to the test set.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mnist-tutorial-nn-data.png&quot; alt=&quot;Data&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 3. demonstrates data preparation and dimensions of X and Y in MNIST training dataset.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Here is a handy code snippet to convert the labels into one-hot embeddings:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-3-helper-functions&quot;&gt;Step 3: Helper functions&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Initialize parameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the weights (W) should have dimensions of \((n^{[l]}, n^{[l-1]})\), where \(n^{[l]}\) is the number of neurons in the current layer and \(n^{[l]}\) is that of the previous one. I recommend applying &lt;a href=&quot;https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78&quot;&gt;He initialization&lt;/a&gt; to optimize the initializer. For the biases, we can initialize them to be zero vectors with the dimension of the current layer.&lt;/p&gt;

&lt;p&gt;Note: The way I set &lt;code&gt;layers = [50, 10]&lt;/code&gt; does not include the dimension of the Input layer (X), therefore, I specify two cases in the code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ReLU function simply outputs Z if Z is positive and 0 otherwise. You can make use of the &lt;code&gt;maximum()&lt;/code&gt; function in numpy library.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Softmax&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We will use Softmax activation to generate the probability for each digit in the 10-dimension vector. We can construct the Softmax activation using the formula:&lt;/p&gt;

\[A^{L}_{i} =  \frac{e^{z^{L}_{i}}}{\sum_{j=1}^{N}e^{z^{L}_{j}}}\]

&lt;p&gt;where \(A^{L}_{i}\) is the probability of each node and K is the number of nodes/classifiers of the last layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compute cost&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The cost function of Softmax for one example (for K classes) is:&lt;/p&gt;

&lt;p&gt;&lt;mark&gt;$$ - \sum_{j=1}^{K} y_j \; log (\hat{y}_j)$$&lt;/mark&gt;&lt;/p&gt;

&lt;p&gt;To calculate the loss, sum the cost of all training examples. This function serves two purposes. Firstly, by printing the cost, we can observe the trend of the model. If the cost does not decrease gradually, this means there are probably some bugs in the code. Secondly, because the cost function fluctuates across iterations, by tracking the training and testing (validating) cost, we can decide on the optimal number of epoch that minimize the cost of testing dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update parameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To keep things simple, in the tutorial, I will apply Gradient Descent to optimize the weights and biases for each layer:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Predict&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Evaluating the accuracy of the model can be as straightforward as run a forward round using the &lt;em&gt;optimal&lt;/em&gt; weights and biases and compare the results to the ground truth labels. Note that since the outputs of the model are 10-dimensional vectors, we need to transform the outcomes to its single-digit form as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-4-forward-path&quot;&gt;Step 4: Forward path&lt;/h3&gt;

&lt;p&gt;To calculate the forward path, we calculate Z and A of layer \([l]\) using the formula:&lt;/p&gt;

\[Z^{[l]} = W^{[l]} \times A^{[l-1]} + b^{[l]}\]

\[A^{[l]} = g^{[l]}(Z^{[l]})\]

&lt;p&gt;where \(g^{[l]}\) is the activation of layer \(l\) (ReLU and Softmax).&lt;/p&gt;

&lt;p&gt;We store Z and A as dictionaries to access them later for the backward path and to make prediction. Remember that our model only has one 50-unit hidden layer and a 10-class softmax layer, we define &lt;code&gt;layers = [50, 10]&lt;/code&gt;. Therefore, we set the first previous &lt;code&gt;A_prev = X&lt;/code&gt;. For the iteration, we will pass in &lt;code&gt;L = len(layers)&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;                                  &lt;span class=&quot;c1&quot;&gt;# Initiate X as A[0]
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;                   &lt;span class=&quot;c1&quot;&gt;# Update A of previous layers
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;            
            &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-5-backward-path&quot;&gt;Step 5: Backward path&lt;/h3&gt;

&lt;p&gt;To update the parameters (W and b), we need to find the derivative of the loss function with respect to each of the parameter. This can be done using the following formulas:&lt;/p&gt;

\[dZ^{[L]} = A^{[L]} - Y\]

\[dZ^{[l]} = dA^{[l]} \times g^{[l]'}(Z^{[l]})\]

\[dW^{[l]} = \frac{1}{m} dZ^{[l]} \times A^{[l-1]}.T\]

\[db^{[l]} = \frac{1}{m} sum(dZ^{[l]})\]

\[dA^{[l]} = W^{[l+1]}.T \times dZ^{[l+1]}\]

&lt;p&gt;Because &lt;code&gt;layers = [50,10]&lt;/code&gt;, &lt;code&gt;range(L)&lt;/code&gt; means to run from 0 to L-1 (i.e. layer 0 (hidden layer) and 1 (output layer)). The reversed sequence will access data from layer L-1 to 0 as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Calculate dZ for the last layer separately from others (as it uses Softmax activation)
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Derivative of Relu activation function = 0 if Z &amp;lt; 0 and = 1 otherwise
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-6-putting-it-all-together&quot;&gt;Step 6: Putting it all together&lt;/h3&gt;

&lt;p&gt;Now we have had all the components required to build the model. The objective is to train the model with training data and use the optimal weights and biases to make prediction for the testing data. To capture the optimal parameters, we choose those that minimize the test set cost. Later, we use these optimal values and run &lt;code&gt;predict&lt;/code&gt; function to calculate Test Accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Side Note:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When training the model for hundreds or thousands of iterations, it is handy to have some visualization assistance which shows a progress bar and estimates the remaining time to complete training. In this post, I imported &lt;blue&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/blue&gt; library for that purpose.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can easily experiment with other fully-connected neural network architectures by changing &lt;code&gt;layers = [ ]&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pbar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                            
        &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Z_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;min_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;min_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;W_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;b_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_description&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Cost Train = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;  -  Cost Test = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Optimal Epoch: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_epoch&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;  -  Test Accuracy = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;wrap-up-highlights&quot;&gt;Wrap-up Highlights&lt;/h2&gt;

&lt;p&gt;Voila! Now you know how to construct a minimal Neural Network to predict handwritten digits. Although our model is simplistic, the results are pretty decent with &lt;strong&gt;94% accuracy&lt;/strong&gt; on the test data. This post is a stepping stone for you to customize your model. In reality, people rarely have to code from scratch thanks to the convenience programming frameworks offer. Hopefully, this sneakpeek into how basic functions work behind the scene can be useful to you in some way.&lt;/p&gt;

&lt;p&gt;Full source code is available on my &lt;a href=&quot;&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;/ol&gt;</content><author><name>Chloe Nguyen</name></author><category term="deeplearning" /><category term="neural-networks" /><category term="deeplearning" /><category term="mnist" /><summary type="html">A neural network = Forward path + Backward path. Simple, isn’t it? Let’s build one from scratch and classify handwritten digits.</summary></entry><entry><title type="html">Simple Linear Regression: Least Squares VS. Gradient Descent</title><link href="http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression.html" rel="alternate" type="text/html" title="Simple Linear Regression: Least Squares VS. Gradient Descent" /><published>2020-11-06T00:04:00+11:00</published><updated>2020-11-06T00:04:00+11:00</updated><id>http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression</id><content type="html" xml:base="http://localhost:4000/supervised-learning/2020/11/06/simple-linear-regression.html">&lt;blockquote&gt;
  &lt;p&gt;This post sheds light on the motivation of Simple Linear Regression and discusses the two optimization methods Least Squares VS. Gradient Descent. Formulas derivation is also included at the end of the post as a bonus.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;
&lt;p&gt;It is human nature to be certain about our future and to make predictions based on existing clues. Hence, the idea of studying about the &lt;strong&gt;association&lt;/strong&gt; among subjects interests us. The first entry of the blog is dedicated to such an idea, it is about one basic concept in supervised learning: &lt;strong&gt;Simple Linear Regression&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;mark&gt;&lt;b&gt;Highlights&lt;/b&gt;&lt;/mark&gt;&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot; id=&quot;markdown-toc-motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#least-squares-method&quot; id=&quot;markdown-toc-least-squares-method&quot;&gt;Least Squares Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-descent-method&quot; id=&quot;markdown-toc-gradient-descent-method&quot;&gt;Gradient Descent Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#model-evaluation&quot; id=&quot;markdown-toc-model-evaluation&quot;&gt;Model Evaluation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#residual-standard-error-rse&quot; id=&quot;markdown-toc-residual-standard-error-rse&quot;&gt;Residual Standard Error (RSE)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#coefficient-of-determination-r2&quot; id=&quot;markdown-toc-coefficient-of-determination-r2&quot;&gt;Coefficient of Determination (\(R^{2}\))&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#key-takeaways&quot; id=&quot;markdown-toc-key-takeaways&quot;&gt;Key Takeaways&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#limitation&quot; id=&quot;markdown-toc-limitation&quot;&gt;Limitation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bonus-least-squares---formula-derivation&quot; id=&quot;markdown-toc-bonus-least-squares---formula-derivation&quot;&gt;BONUS: Least Squares - Formula Derivation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#find-beta_0&quot; id=&quot;markdown-toc-find-beta_0&quot;&gt;Find \(\beta_0\)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#find-beta_1&quot; id=&quot;markdown-toc-find-beta_1&quot;&gt;Find \(\beta_1\)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Simple Linear Regression assumes that the independent \((x)\) and dependent \((y)\) variables have a linear relationship. Our estimation of relationship is summarized in the formula below:&lt;/p&gt;

\[\hat{y} = \beta_0 + \beta_1x\]

&lt;p&gt;\(\beta_0\) is the intercept and \(\beta_1\) is the coefficient (or slope). The name &lt;em&gt;linear regression&lt;/em&gt; suggests that the relationship resembles a straight line.&lt;/p&gt;

&lt;figure class=&quot;width_400&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slr-regplot.png&quot; alt=&quot;SLR Regplot&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 1. Visualization of a Simple Linear Regression problem&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;However, there can hardly be a perfect line that fits all the observations. Hence, errors are expected and we should find a way to minimize them. These errors are called &lt;strong&gt;Residual Sum of Squares (RSS)&lt;/strong&gt; which measures the total of &lt;em&gt;squared&lt;/em&gt; differences between our predictions and the true values (the differences are squared to prevent negative and positive values from cancelling out each other). The formula for RSS is as follows:&lt;/p&gt;

\[RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^{2}\]

&lt;p&gt;In this post, I will discuss two different approaches to find the “perfect” line by optimizing RSS. While Least Squares method provides a &lt;em&gt;one-shot&lt;/em&gt; solution for the problem, Gradient Descent &lt;em&gt;gradually adapts&lt;/em&gt; to it.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/least-squares-vs-gradient-descent.png&quot; alt=&quot;Compare Least Squares and Gradient Descent&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 2. Basic ideas of Least Squares method and Gradient Descent&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;least-squares-method&quot;&gt;Least Squares Method&lt;/h2&gt;

&lt;p&gt;The objective is to find values of \(\beta_0\) and \(\beta_1\) that minimize RSS. We know from calculus that by taking the first derivative of a function (e.g \(f(x)\)), and setting it to 0, we can solve for critical values that minimize \(f(x)\). This method works because the RSS function is convex, therefore, it has one global minimum.&lt;/p&gt;

&lt;figure class=&quot;width_400&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/3d-rss-slr.png&quot; alt=&quot;3D plot of RSS&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 3. 3D plot of Residual Sum of Squares (RSS) (Source: &lt;a href=&quot;http://faculty.marshall.usc.edu/gareth-james/ISL/&quot;&gt;Introduction to Statistical Learning&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The details of how to derive the formulas are included in the &lt;a href=&quot;## BONUS: Least Squares - Formula Derivation&quot;&gt;BONUS Section&lt;/a&gt;. To summarize, we can obtain \(\beta_0\) and \(\beta_1\) using:&lt;/p&gt;

\[\beta_0 = \bar{y} - \beta_1 \bar{x}\]

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}}\]

&lt;p&gt;By plugging in these two parameters, we can now find the perfect line to fit our data and predict future values: \(\hat{y}_i = \beta_0 + \beta_1 x_i\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using Least Squares method, we can calculate the exact values of \(\beta_0\) and \(\beta_1\) using all \(x\) and \(y\) values in &lt;strong&gt;one shot&lt;/strong&gt;. The idea is &lt;strong&gt;straightforward&lt;/strong&gt; and well-known. For Simple Linear Regression, applying this method is effective because we only deal with two variables. This method is suitable for smaller datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Least Squares method is &lt;strong&gt;limited to only a few loss functions&lt;/strong&gt; of certain shapes (e.g. RSS is a convex function). This method is expensive to implement on multivariate problems and mega datasets when &lt;strong&gt;complex matrix operations&lt;/strong&gt; involve (will be discussed later in Multiple Regression module). Therefore, instead of arriving at the bottom of the bowl (i.e. reaching the global minimum) in one go, we can go downhill gradually. That is the inspiration of Gradient Descent.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-method&quot;&gt;Gradient Descent Method&lt;/h2&gt;
&lt;p&gt;Gradient Descent is a more general concept compared to Least Squares as it can be used on various loss functions. In the application of Simple Linear Regression, we can also use Gradient Descent to optimize RSS. The idea of Gradient Descent, as its name suggests, is to update the parameters gradually until the result reaches a certain threshold or completes a desired number of iterations (i.e. &lt;em&gt;epoches&lt;/em&gt;). You can set up a &lt;em&gt;for-loop&lt;/em&gt; in Python to run Gradient Descent using the formula:&lt;/p&gt;

\[\beta_k = \beta_k - \alpha \times \frac{\partial RSS}{\partial \beta_k}  \;\; ; \;\; k =0, 1\]

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gradient Descent works on a &lt;strong&gt;diversity of loss functions&lt;/strong&gt;. Because of its versatility, Gradient Descent is a popular optimization technique used in many machine learning applications. Another benefit is that the computation step of updating the parameter is quite simple (usually the difficult part of complex models is to calculate the derivative of the loss function with respect to the parameters). Therefore, when dealing with multivariate problems such as Multiple Regression for &lt;strong&gt;large datasets&lt;/strong&gt;, Gradient Descent has an edge over Least Square method (by not having to calculate the matrix inverses - we will discuss this in later entries).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/learning-rate-gradient-descent.png&quot; alt=&quot;Learning rates in Gradient Descent&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Fig. 4. The choice of Learning rate can greatly affect Gradient Descent performance.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Choosing the hyperparameter - &lt;strong&gt;learning rate (\(\alpha\)) can be a trouble&lt;/strong&gt; in Gradient Descent. Learning rate determines the step size of each iteration. If \(\alpha\) is too large, there will be lots of deviation and it is difficult to hit the global minimum, or cannot hit the spot at all! If \(\alpha\) is too small, it will take a long time to reach the global minimum. Learning rate is a hyperparameter meaning that we need to allocate a value for it. &lt;em&gt;“Is there a perfect learning rate?”&lt;/em&gt; - you may ask, it &lt;strong&gt;depends on the problem&lt;/strong&gt;, but some popular choices are 0.1, 0.05, 0.01 or smaller values, but we don’t always use a constant learning rate to run the whole model. For instance, in deep learning applications, we usually use learning rate decay (e.g. divide the learning rate in half after every 50 iterations) to prevent the gradients from exploding or vanishing. But for now, let’s look at &lt;em&gt;Fig. 4&lt;/em&gt; to see what different choices of learning rate look like in Simple Linear Regression.&lt;/p&gt;

&lt;h2 id=&quot;model-evaluation&quot;&gt;Model Evaluation&lt;/h2&gt;

&lt;p&gt;After fitting the model, we should check whether we have done a good job or if there are any mistakes. To assess the accuracy of the model, we will discuss two related quantities: Residual Standard Error (RSE) and Coefficient of Determination (\(R^{2}\)) &lt;a class=&quot;citation&quot; href=&quot;#james2013introduction&quot;&gt;(James et al., 2013)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;residual-standard-error-rse&quot;&gt;Residual Standard Error (RSE)&lt;/h3&gt;
&lt;p&gt;RSE is the measurement for the &lt;em&gt;lack of fit&lt;/em&gt;. Although we have applied discussed techniques to achieve the “best” line, we would not be able to fit that line through all data points. This is because the relationship between \(X\) and \(Y\) are not perfectly linear. Comparing the &lt;em&gt;true&lt;/em&gt; equation \(y = \beta_0 + \beta_1x + \epsilon\) to our &lt;em&gt;estimated&lt;/em&gt; equation \(y = \beta_0 + \beta_1x\), it is clear that the error term \(\epsilon\) is not covered.&lt;/p&gt;

\[RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}}\]

&lt;p&gt;If \(y_i \approx \hat{y}_i\), which indicates that our prediction is close to the true value, RSE will be small and we can conclude that our model is a good fit for the data. On the other hand, if the model is not a good fit, \((y_i - \hat{y}_i)^{2}\) will be large and so as RSE.&lt;/p&gt;

&lt;h3 id=&quot;coefficient-of-determination-r2&quot;&gt;Coefficient of Determination (\(R^{2}\))&lt;/h3&gt;
&lt;p&gt;On the other end of the spectrum, \(R^{2}\) is the measurement of &lt;em&gt;model good fit&lt;/em&gt;. It can be interpreted as &lt;strong&gt;the amount of variations in \(Y\) that can be explained by \(X\)&lt;/strong&gt; using our model.&lt;/p&gt;

\[R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}}{\sum_{i=1}^{n}(y_i - \bar{y})^{2}}\]

&lt;p&gt;RSS is the variations left in \(Y\) after fitting our model, this is how much the model is lacking. TSS is the total variations (or the natural variations) in \(Y\) before fitting the model. Therefore, \(R^2\) shows how much variations in \(Y\) are resolved by our model. \(R^2\) score lies between 0 and 1. For example, \(R^2 = 0.75\) means that \(75\%\) of the variations in \(Y\) is explained by our model. A higher value of \(R^2\) indicates great goodness of fit and vice versa.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;h3 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h3&gt;
&lt;p&gt;In this post, we cover the core ideas of Simple Linear Regression and discuss two methods to optimize this model. The Least Squares method is straightforward and allows us to find \(\beta_0\) and \(\beta_1\) directly using two formulas. Gradient Descent can be used for a diversity of loss functions and can arrive at the solution gradually as long as we set a reasonable learning rate.&lt;/p&gt;

&lt;p&gt;After optimizing the model, we can assess its accuracy using RSE and \(R^2\), which represents the lack of fit and goodness of fit respectively. Thus, a good model should have low \(RSE\) and high \(R^2\).&lt;/p&gt;

&lt;h3 id=&quot;limitation&quot;&gt;Limitation&lt;/h3&gt;
&lt;p&gt;Though the simplicity of the model is a good point, Simple Linear Regression has many shortcomings. Today, we will briefly look at two major limitations. Firstly, Simple Linear Regression has an oversimplified assumption. In reality, many relationships are non-linear. Hence, forcing the relationship to be linear will not produce accurate predictions. Secondly, there can be many contributing factors to a problem and they can also have some interaction effects. Therefore, instead of having multiple Simple Linear Regression models, there should be an upgrade that takes into account the &lt;em&gt;synergy&lt;/em&gt; among variables. To answer that, we will consider &lt;strong&gt;Multiple Linear Regression&lt;/strong&gt; in the next entry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chloe’s End Note&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Thank you so much for checking out my first post. Please let me know if you have any ideas on improving this post. As a bonus, I have included the details on how to derive Least Squares formulas. Hope that they will be helpful to someone. Also, stay tuned for my future posts :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;bonus-least-squares---formula-derivation&quot;&gt;BONUS: Least Squares - Formula Derivation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Find values of \(\beta_0\) and \(\beta_1\) that minimize RSS:&lt;/p&gt;

\[RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2} = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^{2}\]

&lt;h3 id=&quot;find-beta_0&quot;&gt;Find \(\beta_0\)&lt;/h3&gt;
&lt;p&gt;First, find the partial derivative of RSS with respect to \(\beta_0\) treating \(x, y\) and \(\beta_1\) as constants:&lt;/p&gt;

\[\frac{\partial RSS}{\partial \beta_0} = -2 \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i)\]

&lt;p&gt;Next, we set this value to 0 and solve for \(\beta_0\):&lt;/p&gt;

\[\begin{aligned}
-2 \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i) &amp;amp; = 0 \\
\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i) &amp;amp; = 0 \\
\sum_{i=1}^{n}y_i - n\beta_0 - n\beta_1 \sum_{i=1}^{n}x_i &amp;amp; = 0 \\
\end{aligned}\]

\[\beta_0 = \frac{\sum_{i=1}^{n}y_i - \beta_1 \sum_{i=1}^{n}x_i}{n}\]

&lt;p&gt;Because:  \(\frac{\sum_{i=1}^{n}x_i}{n} = \bar{x}\)   and    \(\frac{\sum_{i=1}^{n}y_i}{n} = \bar{y}\):&lt;/p&gt;

\[{\color{Blue} {\beta_0 = \bar{y} - \beta_1 \bar{x}}}\]

&lt;h3 id=&quot;find-beta_1&quot;&gt;Find \(\beta_1\)&lt;/h3&gt;
&lt;p&gt;Similarly, we find the partial derivative of RSS with respect to \(\beta_1\) and set it to 0:&lt;/p&gt;

\[\begin{aligned}
\frac{\partial RSS}{\partial \beta_1} = -2 \sum_{i=1}^{n}x_i(y_i - \beta_0 - \beta_1x_i) &amp;amp; = 0 \\
\sum_{i=1}^{n}x_iy_i - \beta_0 \sum_{i=1}^{n}x_i - \beta_1 \sum_{i=1}^{n}x_i^{2} &amp;amp; = 0
\end{aligned}\]

&lt;p&gt;Plug in \(\beta_0 = \bar{y} - \beta_1 \bar{x}\):&lt;/p&gt;

\[\begin{aligned}
\sum_{i=1}^{n}x_iy_i - (\bar{y} - \beta_1 \bar{x})\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^{2} &amp;amp; = 0 \\
\sum_{i=1}^{n}x_iy_i - \bar{y}\sum_{i=1}^{n}x_i + \beta_1 \bar{x}\sum_{i=1}^{n}x_i - \beta_1\sum_{i=1}^{n}x_i^{2} &amp;amp; = 0 \\
\sum_{i=1}^{n}x_i(y_i - \bar{y}) - \beta_1 \sum_{i=1}^{n}x_i (x_i - \bar{x}) &amp;amp; = 0 \\
\end{aligned}\]

&lt;p&gt;Solve for \(\beta_1\):&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x})}\]

&lt;p&gt;We can conclude here and use this equation to find \(\beta_1\), but the more widely used variation in books and lecture notes is:&lt;/p&gt;

\[\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}} = \frac{S_{XY}}{S_{XX}}\]

&lt;p&gt;Many people find the equation above easier to remember as its numerator \((S_{XY})\) and denominator \((S_{XX})\) are the outputs of some software functions. With a few algebra adjustments, we can transform our original equation into its well-known form:&lt;/p&gt;

\[\begin{aligned}
\beta_1 
&amp;amp;= \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x})} \\
&amp;amp;= \frac{\sum_{i=1}^{n}x_i(y_i - \bar{y}) - \sum_{i=1}^{n} \bar{x} (y_i - \bar{y}) + \sum_{i=1}^{n} \bar{x} (y_i - \bar{y})}{\sum_{i=1}^{n}x_i (x_i - \bar{x}) - \sum_{i=1}^{n}\bar{x}(x_i - \bar{x}) + \sum_{i=1}^{n}\bar{x}(x_i - \bar{x})} \\
&amp;amp;= \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y}) + \bar{x} (\sum_{i=1}^{n}y_i - n\bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2} + \bar{x} (\sum_{i=1}^{n}x_i - n\bar{x})}
\end{aligned}\]

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In the last term of the equation in the numerator, (1) \(\bar{x}\) is a number, so it can come out of the sum and (2) \(\sum_{i=1}^{n}y_i = n\bar{y}\). Therefore, \(\bar{x} (\sum_{i=1}^{n}y_i - n\bar{y}) = 0\). The same thing applies to the denominator, we have the final equation:&lt;/p&gt;

\[{\color{Blue} {\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^{2}}}}\]

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;james2013introduction&quot;&gt;James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2013). &lt;i&gt;An introduction to statistical learning&lt;/i&gt; (Vol. 112). Springer.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Chloe Nguyen</name></author><category term="supervised-learning" /><category term="regression" /><category term="least-squares" /><category term="gradient-descent" /><summary type="html">This post sheds light on the motivation of Simple Linear Regression and discusses the two optimization methods Least Squares VS. Gradient Descent. Formulas derivation is also included at the end of the post as a bonus.</summary></entry></feed>